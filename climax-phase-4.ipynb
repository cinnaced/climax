{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":13299687,"sourceType":"datasetVersion","datasetId":8429777},{"sourceId":13303078,"sourceType":"datasetVersion","datasetId":8432230}],"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# üåç ClimAx Phase 4 - Complete Multi-Scenario Training\n\n**Complete notebook for training ClimAx models across multiple climate scenarios**\n\n---\n\n## üìã Features\n- ‚úÖ Multi-scenario training (historical + SSP126/245/370/585)\n- ‚úÖ Stratified splitting (all scenarios in train/val/test)\n- ‚úÖ Memory-efficient for 6GB GPU\n- ‚úÖ Progress bars with tqdm\n- ‚úÖ Automatic checkpointing\n\n## üéØ Requirements\n- PyTorch with CUDA\n- 6GB+ GPU memory\n- Climate data in E:/Datasets/ (or update paths)\n\n---","metadata":{}},{"cell_type":"markdown","source":"## üì¶ Section 1: Install & Import Dependencies","metadata":{}},{"cell_type":"code","source":"# =============================================================================\n# üì¶ SECTION 1: INSTALL & IMPORT DEPENDENCIES - FIXED\n# =============================================================================\n\n# Install required packages (uncomment if needed on Kaggle)\n# !pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n# !pip install xarray netcdf4 scipy scikit-learn tqdm matplotlib\n\nimport os\nimport sys  # ‚úÖ CRITICAL: Added for sys.stdout.flush()\nimport time\nimport json\nimport logging\nimport gc\nimport glob\nimport math\nfrom datetime import datetime\nfrom typing import Dict, List, Tuple, Optional, Any\n\nimport numpy as np\nimport xarray as xr\nfrom scipy.ndimage import zoom\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.utils.checkpoint import checkpoint\n\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Setup logging with more detail\nlogging.basicConfig(\n    level=logging.INFO,\n    format=\"%(asctime)s [%(levelname)s] %(message)s\"\n)\nlogger = logging.getLogger(\"phase4_training\")\n\nprint(\"‚úÖ All imports successful!\")\nprint(f\"üéÆ PyTorch version: {torch.__version__}\")\nprint(f\"üéÆ CUDA available: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"üéÆ GPU: {torch.cuda.get_device_name(0)}\")\n    print(f\"üéÆ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / (1024**3):.1f} GB\")\nelse:\n    print(\"‚ö†Ô∏è WARNING: No GPU detected - training will be VERY slow!\")\n\n# Force flush output immediately\nsys.stdout.flush()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-09T09:57:58.395218Z","iopub.execute_input":"2025-10-09T09:57:58.395824Z","iopub.status.idle":"2025-10-09T09:58:04.057368Z","shell.execute_reply.started":"2025-10-09T09:57:58.395799Z","shell.execute_reply":"2025-10-09T09:58:04.056583Z"}},"outputs":[{"name":"stdout","text":"‚úÖ All imports successful!\nüéÆ PyTorch version: 2.6.0+cu124\nüéÆ CUDA available: True\nüéÆ GPU: Tesla P100-PCIE-16GB\nüéÆ GPU Memory: 15.9 GB\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"## ‚öôÔ∏è Section 2: Configuration\n\n**‚ö†Ô∏è UPDATE THE DATA PATHS BELOW!**","metadata":{}},{"cell_type":"code","source":"# =============================================================================\n# üåç CONFIGURATION - KAGGLE PATHS\n# =============================================================================\n\n# ‚úÖ Updated for Kaggle dataset structure\nINPUT_DATA_DIR = \"/kaggle/input/climate-dataset/Datasets/inputs/input4mips\"\nOUTPUT_DATA_DIR = \"/kaggle/input/climate-dataset/Datasets/outputs/CMIP6\"\nOUTPUT_DIR = \"climax_phase4_results\"\n\n# Training settings\nSMOKE_TEST = True              # True = 2 institutions, 2 epochs for testing\nRESUME_IF_MODEL_EXISTS = True  # Skip already trained models\nSKIP_TRAINING = False          # Set True to only load results\n\n# Memory settings\nTARGET_SPATIAL_H = 9\nTARGET_SPATIAL_W = 19\nBATCH_SIZE = 1\nUSE_CHECKPOINTING = True\nEPOCHS = 2 if SMOKE_TEST else 15  # Short for smoke test\n\n# Model settings\nEMBED_DIM = 64\nDEPTH = 4\nNUM_HEADS = 4\n\n# All 18 institutions\nALL_INSTITUTIONS = [\n    \"AWI-CM-1-1-MR\", \"BCC-CSM2-MR\", \"CAS-ESM2-0\", \"CESM2\",\n    \"CESM2-WACCM\", \"CMCC-CM2-SR5\", \"CMCC-ESM2\", \"CNRM-CM6-1-HR\",\n    \"EC-Earth3\", \"EC-Earth3-Veg\", \"EC-Earth3-Veg-LR\", \"FGOALS-f3-L\",\n    \"GFDL-ESM4\", \"INM-CM4-8\", \"INM-CM5-0\", \"MPI-ESM1-2-HR\",\n    \"MRI-ESM2-0\", \"TaiESM1\"\n]\n\nSCENARIOS = {\n    'historical': 0,\n    'ssp126': 1,\n    'ssp245': 2,\n    'ssp370': 3,\n    'ssp585': 4\n}\n\nprint(\"‚úÖ Configuration loaded\")\nprint(f\"   üìÅ Input: {INPUT_DATA_DIR}\")\nprint(f\"   üìÅ Output: {OUTPUT_DATA_DIR}\")\nprint(f\"   üíæ Results: /kaggle/working/{OUTPUT_DIR}\")\nprint(f\"   üó∫Ô∏è  Spatial: {TARGET_SPATIAL_H}√ó{TARGET_SPATIAL_W}\")\nprint(f\"   üè¢ Institutions: {len(ALL_INSTITUTIONS)}\")\nprint(f\"   üåç Scenarios: {list(SCENARIOS.keys())}\")\nprint(f\"   ‚öôÔ∏è  Smoke test: {SMOKE_TEST}\")\n\n# Verify paths exist\nif not os.path.exists(INPUT_DATA_DIR):\n    print(f\"\\n‚ùå ERROR: Input path not found!\")\n    print(f\"   Path: {INPUT_DATA_DIR}\")\n    print(f\"   Make sure 'climate-dataset' is attached to this notebook!\")\nelse:\n    print(f\"\\n‚úÖ Input path verified\")\n\nif not os.path.exists(OUTPUT_DATA_DIR):\n    print(f\"\\n‚ùå ERROR: Output path not found!\")\n    print(f\"   Path: {OUTPUT_DATA_DIR}\")\n    print(f\"   Make sure 'climate-dataset' is attached to this notebook!\")\nelse:\n    print(f\"‚úÖ Output path verified\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-09T09:58:04.058824Z","iopub.execute_input":"2025-10-09T09:58:04.059213Z","iopub.status.idle":"2025-10-09T09:58:04.080694Z","shell.execute_reply.started":"2025-10-09T09:58:04.059196Z","shell.execute_reply":"2025-10-09T09:58:04.079474Z"}},"outputs":[{"name":"stdout","text":"‚úÖ Configuration loaded\n   üìÅ Input: /kaggle/input/climate-dataset/Datasets/inputs/input4mips\n   üìÅ Output: /kaggle/input/climate-dataset/Datasets/outputs/CMIP6\n   üíæ Results: /kaggle/working/climax_phase4_results\n   üó∫Ô∏è  Spatial: 9√ó19\n   üè¢ Institutions: 18\n   üåç Scenarios: ['historical', 'ssp126', 'ssp245', 'ssp370', 'ssp585']\n   ‚öôÔ∏è  Smoke test: True\n\n‚úÖ Input path verified\n‚úÖ Output path verified\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# =============================================================================\n# üîç DEBUG: Inspect Data Structure\n# =============================================================================\n\nprint(\"=\"*80)\nprint(\"üîç DEBUGGING: Checking actual file structure\")\nprint(\"=\"*80)\n\n# Check inputs structure\nprint(\"\\nüìä INPUT STRUCTURE:\")\ninput_base = \"/kaggle/input/climate-dataset/Datasets/inputs/input4mips\"\n\nif os.path.exists(input_base):\n    scenarios = os.listdir(input_base)\n    print(f\"‚úÖ Found scenarios: {scenarios}\\n\")\n    \n    # Check historical scenario as example\n    hist_path = os.path.join(input_base, \"historical\")\n    if os.path.exists(hist_path):\n        print(f\"üìÅ Contents of historical/:\")\n        for item in os.listdir(hist_path):\n            item_path = os.path.join(hist_path, item)\n            if os.path.isdir(item_path):\n                # Count files in this directory\n                files = []\n                for root, dirs, filenames in os.walk(item_path):\n                    files.extend([f for f in filenames if f.endswith('.nc')])\n                print(f\"   üìÇ {item}/ ‚Üí {len(files)} .nc files\")\n    else:\n        print(\"‚ùå historical/ not found\")\nelse:\n    print(f\"‚ùå Path not found: {input_base}\")\n\n# Check outputs structure\nprint(\"\\nüéØ OUTPUT STRUCTURE:\")\noutput_base = \"/kaggle/input/climate-dataset/Datasets/outputs/CMIP6\"\n\nif os.path.exists(output_base):\n    institutions = os.listdir(output_base)\n    print(f\"‚úÖ Found {len(institutions)} institutions\\n\")\n    \n    # Check first institution as example\n    first_inst = institutions[0] if institutions else None\n    if first_inst:\n        inst_path = os.path.join(output_base, first_inst)\n        print(f\"üìÅ Contents of {first_inst}/:\")\n        \n        if os.path.isdir(inst_path):\n            items = os.listdir(inst_path)\n            print(f\"   Items: {items[:5]}...\")  # Show first 5\n            \n            # Check if scenarios are inside\n            for scenario in ['historical', 'ssp126', 'ssp245']:\n                scenario_path = os.path.join(inst_path, scenario)\n                if os.path.exists(scenario_path):\n                    print(f\"   ‚úì {scenario}/ exists\")\n                    # Check for pr variable\n                    pr_path = os.path.join(scenario_path, \"pr\")\n                    if os.path.exists(pr_path):\n                        files = []\n                        for root, dirs, filenames in os.walk(pr_path):\n                            files.extend([f for f in filenames if f.endswith('.nc')])\n                        print(f\"      ‚Üí pr/ ‚Üí {len(files)} .nc files\")\n                else:\n                    print(f\"   ‚úó {scenario}/ not found\")\nelse:\n    print(f\"‚ùå Path not found: {output_base}\")\n\nprint(\"\\n\" + \"=\"*80)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-09T09:58:04.081626Z","iopub.execute_input":"2025-10-09T09:58:04.081934Z","iopub.status.idle":"2025-10-09T09:58:13.700717Z","shell.execute_reply.started":"2025-10-09T09:58:04.081910Z","shell.execute_reply":"2025-10-09T09:58:13.699995Z"}},"outputs":[{"name":"stdout","text":"================================================================================\nüîç DEBUGGING: Checking actual file structure\n================================================================================\n\nüìä INPUT STRUCTURE:\n‚úÖ Found scenarios: ['ssp585', 'ssp370', 'historical', 'ssp126', 'ssp245']\n\nüìÅ Contents of historical/:\n   üìÇ CO2_sum/ ‚Üí 165 .nc files\n   üìÇ SO2_sum/ ‚Üí 495 .nc files\n   üìÇ BC_sum/ ‚Üí 495 .nc files\n   üìÇ CH4_sum/ ‚Üí 495 .nc files\n\nüéØ OUTPUT STRUCTURE:\n‚úÖ Found 18 institutions\n\nüìÅ Contents of CMCC-CM2-SR5/:\n   Items: ['ssp585', 'ssp370', 'historical', 'ssp126', 'ssp245']...\n   ‚úì historical/ exists\n      ‚Üí pr/ ‚Üí 165 .nc files\n   ‚úì ssp126/ exists\n      ‚Üí pr/ ‚Üí 86 .nc files\n   ‚úì ssp245/ exists\n      ‚Üí pr/ ‚Üí 86 .nc files\n\n================================================================================\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"class Phase4Config:\n    \"\"\"Minimal config for Phase 4 - FIXED for scenario channels\"\"\"\n    def __init__(self):\n        self.INPUT_DATA_DIR = INPUT_DATA_DIR\n        self.OUTPUT_DATA_DIR = OUTPUT_DATA_DIR\n        self.SPATIAL_HEIGHT = TARGET_SPATIAL_H\n        self.SPATIAL_WIDTH = TARGET_SPATIAL_W\n        self.SPATIAL_H = TARGET_SPATIAL_H\n        self.SPATIAL_W = TARGET_SPATIAL_W\n        self.PATCH_SIZE = 1\n        self.EMBED_DIM = EMBED_DIM\n        self.DEPTH = DEPTH\n        self.NUM_HEADS = NUM_HEADS\n        self.MLP_RATIO = 4.0\n        self.DROPOUT_RATE = 0.1\n        self.ATTENTION_DROPOUT = 0.1\n        self.DROP_PATH_RATE = 0.1\n        self.SEQUENCE_INPUT_LENGTH = 12\n        self.SEQUENCE_OUTPUT_LENGTH = 3\n        self.TEMPORAL_STRIDE = 1\n        self.BATCH_SIZE = BATCH_SIZE\n        self.TRAIN_RATIO = 0.7\n        self.VAL_RATIO = 0.15\n        self.LEARNING_RATE = 1e-4\n        self.WEIGHT_DECAY = 0.05\n        self.CLIP_GRAD_NORM = 1.0\n        \n        # Physical input variables (from files only)\n        self.INPUT_VARIABLES = [\n            'BC_anthro_fires', 'BC_no_fires',\n            'CH4_anthro_fires', 'CH4_no_fires',\n            'CO2_sum',\n            'SO2_anthro_fires', 'SO2_no_fires'\n        ]\n        \n        # ‚úÖ FIXED: Total input channels (7 variables + 1 scenario channel)\n        self.NUM_INPUT_CHANNELS = 8\n        \n        self.OUTPUT_VARIABLE = 'pr'\n\n# Create directories\nos.makedirs(OUTPUT_DIR, exist_ok=True)\nos.makedirs(os.path.join(OUTPUT_DIR, \"checkpoints\"), exist_ok=True)\nos.makedirs(os.path.join(OUTPUT_DIR, \"logs\"), exist_ok=True)\nos.makedirs(os.path.join(OUTPUT_DIR, \"plots\"), exist_ok=True)\n\nprint(\"‚úÖ Config class created\")\nprint(\"‚úÖ Output directories created\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-09T09:58:13.701407Z","iopub.execute_input":"2025-10-09T09:58:13.701678Z","iopub.status.idle":"2025-10-09T09:58:13.709422Z","shell.execute_reply.started":"2025-10-09T09:58:13.701658Z","shell.execute_reply":"2025-10-09T09:58:13.708868Z"}},"outputs":[{"name":"stdout","text":"‚úÖ Config class created\n‚úÖ Output directories created\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"## üìä Section 3: Data Loader (Stratified Multi-Scenario)","metadata":{}},{"cell_type":"code","source":"# =============================================================================\n# üìä SECTION 3: DATA LOADER - FIXED VERSION (From working Python file)\n# =============================================================================\n\nclass ClimateDatasetWithScenario(Dataset):\n    \"\"\"PyTorch dataset with scenario information\"\"\"\n    \n    def __init__(self, X: np.ndarray, y: np.ndarray, scenarios: np.ndarray):\n        self.X = torch.from_numpy(X.astype(np.float32))\n        self.y = torch.from_numpy(y.astype(np.float32))\n        self.scenarios = torch.from_numpy(scenarios.astype(np.float32))\n    \n    def __len__(self):\n        return self.X.shape[0]\n    \n    def __getitem__(self, idx):\n        return self.X[idx], self.y[idx], self.scenarios[idx]\n\n\nclass Phase4MultiScenarioDataLoader:\n    \"\"\"\n    Multi-Scenario Data Loader - FIXED VERSION from working Python file\n    ‚úÖ Robust file discovery\n    ‚úÖ Better error handling\n    ‚úÖ Memory-efficient\n    \"\"\"\n    \n    def __init__(self, config, target_h: int = 9, target_w: int = 19):\n        self.input_base = config.INPUT_DATA_DIR\n        self.output_base = config.OUTPUT_DATA_DIR\n        self.target_h = target_h\n        self.target_w = target_w\n        self.scalers = {}\n        \n        self.scenarios = {'historical': 0, 'ssp126': 1, 'ssp245': 2, 'ssp370': 3, 'ssp585': 4}\n        self.input_variables = config.INPUT_VARIABLES\n        self.variable_dirs = {\n            'BC_anthro_fires': 'BC_sum', 'BC_no_fires': 'BC_sum',\n            'CH4_anthro_fires': 'CH4_sum', 'CH4_no_fires': 'CH4_sum',\n            'CO2_sum': 'CO2_sum',\n            'SO2_anthro_fires': 'SO2_sum', 'SO2_no_fires': 'SO2_sum'\n        }\n        self.output_variable = config.OUTPUT_VARIABLE\n        logger.info(f\"üìä DataLoader initialized: {target_h}√ó{target_w}\")\n    \n    def _matches_variable_pattern(self, filename: str, variable: str) -> bool:\n        \"\"\"Pattern matching for filenames\"\"\"\n        f = filename.lower()\n        patterns = {\n            'BC_anthro_fires': (['bc', 'anthro'], ['fire']),\n            'BC_no_fires': (['bc', 'no'], ['fire']),\n            'CH4_anthro_fires': (['ch4', 'anthro'], ['fire']),\n            'CH4_no_fires': (['ch4', 'no'], ['fire']),\n            'CO2_sum': (['co2'], []),\n            'SO2_anthro_fires': (['so2', 'anthro'], ['fire']),\n            'SO2_no_fires': (['so2', 'no'], ['fire'])\n        }\n        \n        if variable in patterns:\n            req, opt = patterns[variable]\n            has_req = all(p in f for p in req)\n            if 'no' in req:\n                has_req = has_req and 'anthro' not in f\n            if opt:\n                return has_req and any(p in f for p in opt)\n            return has_req\n        return False\n    \n    def _is_file_readable(self, file_path: str) -> bool:\n        \"\"\"Check if file can be opened - CRITICAL FIX\"\"\"\n        try:\n            if not os.path.exists(file_path) or os.path.getsize(file_path) < 1000:\n                return False\n            # Quick check without fully loading\n            with xr.open_dataset(file_path, decode_times=False) as ds:\n                _ = list(ds.data_vars.keys())\n            return True\n        except:\n            return False\n    \n    def discover_files_multi_scenario(self, institution: str):\n        \"\"\"Discover files for all scenarios - FIXED with better logging\"\"\"\n        logger.info(f\"üîç Discovering files for {institution}...\")\n        all_scenario_files = {}\n        \n        for scenario in self.scenarios.keys():\n            logger.info(f\"   üìÅ {scenario}\")\n            results_inputs = {}\n            results_outputs = {}\n            \n            # Input files\n            for var in self.input_variables:\n                folder = self.variable_dirs.get(var, var)\n                path = os.path.join(self.input_base, scenario, folder)\n                \n                if not os.path.exists(path):\n                    logger.warning(f\"      ‚ö†Ô∏è Path not found: {path}\")\n                    continue\n                \n                found = []\n                try:\n                    for root, dirs, files in os.walk(path):\n                        for fname in files:\n                            if fname.endswith('.nc') and self._matches_variable_pattern(fname, var):\n                                fpath = os.path.join(root, fname)\n                                if self._is_file_readable(fpath):\n                                    found.append(fpath)\n                                    if len(found) % 10 == 0:  # Progress update\n                                        logger.info(f\"         Found {len(found)} {var} files...\")\n                except Exception as e:\n                    logger.error(f\"      ‚ùå Error walking {path}: {e}\")\n                    continue\n                \n                if found:\n                    results_inputs[var] = found\n                    logger.info(f\"      ‚úì {var}: {len(found)} files\")\n                else:\n                    logger.warning(f\"      ‚ö†Ô∏è {var}: No readable files found\")\n            \n            # Output files\n            out_path = os.path.join(self.output_base, institution, scenario, self.output_variable)\n            if os.path.exists(out_path):\n                out_files = []\n                try:\n                    for root, dirs, files in os.walk(out_path):\n                        for fname in files:\n                            if fname.endswith('.nc'):\n                                fpath = os.path.join(root, fname)\n                                if self._is_file_readable(fpath):\n                                    out_files.append(fpath)\n                                    if len(out_files) % 10 == 0:\n                                        logger.info(f\"         Found {len(out_files)} output files...\")\n                except Exception as e:\n                    logger.error(f\"      ‚ùå Error walking {out_path}: {e}\")\n                \n                if out_files:\n                    results_outputs[self.output_variable] = out_files\n                    logger.info(f\"      ‚úì {self.output_variable}: {len(out_files)} files\")\n            else:\n                logger.warning(f\"      ‚ö†Ô∏è Output path not found: {out_path}\")\n            \n            all_scenario_files[scenario] = {\"inputs\": results_inputs, \"outputs\": results_outputs}\n        \n        return all_scenario_files\n    \n    def downsample_spatial(self, arr, target_h, target_w):\n        \"\"\"Downsample using scipy.ndimage.zoom\"\"\"\n        if arr.shape[1] == target_h and arr.shape[2] == target_w:\n            return arr\n        T, H, W = arr.shape\n        downsampled = zoom(arr, [1.0, target_h/H, target_w/W], order=1, mode='nearest')\n        return downsampled[:, :target_h, :target_w]\n    \n    def _load_netcdf_list(self, paths, var_hint=None):\n        \"\"\"Load and concatenate NetCDF files\"\"\"\n        arrays = []\n        for p in sorted(paths):\n            try:\n                ds = xr.open_dataset(p, decode_times=False)\n                dvars = list(ds.data_vars.keys())\n                if not dvars:\n                    ds.close()\n                    continue\n                var = var_hint if (var_hint and var_hint in dvars) else dvars[0]\n                arr = ds[var].values\n                ds.close()\n                if arr.ndim == 2:\n                    arr = np.expand_dims(arr, 0)\n                elif arr.ndim > 3:\n                    arr = arr.reshape(-1, arr.shape[-2], arr.shape[-1])\n                arrays.append(np.nan_to_num(arr, 0.0, 0.0, 0.0))\n            except Exception as e:\n                logger.warning(f\"‚ö†Ô∏è Error reading {os.path.basename(p)}: {e}\")\n        \n        if not arrays:\n            return np.zeros((0, 0, 0), dtype=float)\n        \n        try:\n            return np.concatenate(arrays, axis=0)\n        except:\n            mh = max(a.shape[1] for a in arrays)\n            mw = max(a.shape[2] for a in arrays)\n            padded = [np.pad(a, ((0,0), (0,mh-a.shape[1]), (0,mw-a.shape[2])), 'edge') for a in arrays]\n            return np.concatenate(padded, axis=0)\n    \n    def align_temporal_dimensions(self, var_data):\n        \"\"\"Align temporal dimensions\"\"\"\n        if not var_data:\n            return var_data\n        times = {v: a.shape[0] for v, a in var_data.items()}\n        mt = min(times.values())\n        if mt != max(times.values()):\n            logger.warning(f\"‚ö†Ô∏è Aligning to {mt} timesteps\")\n            return {v: a[:mt] for v, a in var_data.items()}\n        return var_data\n    \n    def load_all_scenarios(self, all_files):\n        \"\"\"Load all scenario data\"\"\"\n        logger.info(\"üìä Loading scenarios...\")\n        all_data = {}\n        \n        for scenario, files in all_files.items():\n            logger.info(f\"   üìÅ {scenario}\")\n            var_data = {}\n            \n            for var, paths in files.get(\"inputs\", {}).items():\n                if paths:\n                    logger.info(f\"      Loading {var} ({len(paths)} files)...\")\n                    arr = self._load_netcdf_list(paths)\n                    if arr.size > 0:\n                        var_data[var] = arr\n                        logger.info(f\"      ‚úì {var}: {arr.shape}\")\n            \n            for var, paths in files.get(\"outputs\", {}).items():\n                if paths:\n                    logger.info(f\"      Loading {var} ({len(paths)} files)...\")\n                    arr = self._load_netcdf_list(paths)\n                    if arr.size > 0:\n                        var_data[var] = arr\n                        logger.info(f\"      ‚úì {var}: {arr.shape}\")\n            \n            if var_data:\n                var_data = self.align_temporal_dimensions(var_data)\n                logger.info(f\"      üîΩ Downsampling...\")\n                down_data = {}\n                for v, a in var_data.items():\n                    d = self.downsample_spatial(a, self.target_h, self.target_w)\n                    down_data[v] = d\n                    logger.info(f\"         {v}: {a.shape} ‚Üí {d.shape}\")\n                all_data[scenario] = down_data\n        \n        logger.info(f\"   ‚úÖ Loaded {len(all_data)} scenarios\")\n        return all_data\n    \n    def normalize_data(self, arr, var_name, fit=True):\n        \"\"\"Normalize data\"\"\"\n        arr = np.nan_to_num(arr, 0.0, 0.0, 0.0)\n        flat = arr.reshape(-1, 1)\n        \n        if fit or var_name not in self.scalers:\n            scaler = MinMaxScaler()\n            try:\n                scaler.fit(flat)\n            except:\n                scaler.min_, scaler.scale_ = np.min(flat), 1.0\n            self.scalers[var_name] = scaler\n        else:\n            scaler = self.scalers[var_name]\n        \n        return np.nan_to_num(scaler.transform(flat).reshape(arr.shape), 0.0, 0.0, 0.0)\n    \n    def create_multi_scenario_sequences(self, all_data, seq_in=12, seq_out=3, stride=1, train_ratio=0.7, val_ratio=0.15, batch_size=1):\n        \"\"\"Create sequences with stratified splitting\"\"\"\n        logger.info(\"üîÑ Creating sequences...\")\n        X_seqs, Y_seqs, sc_ids = [], [], []\n        \n        for scenario, var_data in all_data.items():\n            if not var_data:\n                continue\n            \n            sc_id = self.scenarios[scenario]\n            logger.info(f\"   üì¶ {scenario} (id={sc_id})\")\n            \n            missing = [v for v in self.input_variables if v not in var_data]\n            if missing or self.output_variable not in var_data:\n                logger.warning(f\"      ‚ö†Ô∏è Skipping - missing data\")\n                continue\n            \n            # Normalize\n            norm = {v: self.normalize_data(var_data[v], v, True) for v in self.input_variables if v in var_data}\n            norm[self.output_variable] = self.normalize_data(var_data[self.output_variable], self.output_variable, True)\n            \n            # Stack\n            X_sc = np.stack([norm[v] for v in self.input_variables if v in norm], axis=1)\n            Y_sc = norm[self.output_variable]\n            \n            T = X_sc.shape[0]\n            n_samp = T - seq_in - seq_out + 1\n            \n            if n_samp <= 0:\n                logger.warning(f\"      ‚ö†Ô∏è Not enough timesteps\")\n                continue\n            \n            for start in range(0, n_samp, stride):\n                X_seqs.append(X_sc[start:start+seq_in])\n                Y_seqs.append(Y_sc[start+seq_in:start+seq_in+seq_out])\n                sc_ids.append(sc_id)\n            \n            logger.info(f\"      ‚úì {n_samp} sequences\")\n        \n        if not X_seqs:\n            raise RuntimeError(\"‚ùå No sequences!\")\n        \n        X_all = np.stack(X_seqs, 0)\n        Y_all = np.stack(Y_seqs, 0)\n        sc_all = np.array(sc_ids)\n        \n        # Add scenario channel\n        N, T, C, H, W = X_all.shape\n        sc_ch = np.repeat(sc_all[:, None, None, None, None], T*H*W, 1).reshape(N, T, 1, H, W)\n        X_all = np.concatenate([X_all, sc_ch], 2)\n        Y_all = np.expand_dims(Y_all, 2)\n        \n        logger.info(f\"   üß© Total: X={X_all.shape}, Y={Y_all.shape}\")\n        \n        # Stratified split\n        logger.info(\"   üìä Stratified split...\")\n        X_tr, X_tmp, y_tr, y_tmp, sc_tr, sc_tmp = train_test_split(\n            X_all, Y_all, sc_all, test_size=(1-train_ratio), stratify=sc_all, random_state=42\n        )\n        vt_ratio = val_ratio / (1 - train_ratio)\n        X_val, X_te, y_val, y_te, sc_val, sc_te = train_test_split(\n            X_tmp, y_tmp, sc_tmp, test_size=(1-vt_ratio), stratify=sc_tmp, random_state=42\n        )\n        \n        splits = {\n            \"train\": {\"input\": X_tr, \"target\": y_tr, \"scenarios\": sc_tr},\n            \"validation\": {\"input\": X_val, \"target\": y_val, \"scenarios\": sc_val},\n            \"test\": {\"input\": X_te, \"target\": y_te, \"scenarios\": sc_te}\n        }\n        \n        # CRITICAL FIX: pin_memory=False for Kaggle\n        loaders = {\n            \"train\": DataLoader(ClimateDatasetWithScenario(X_tr, y_tr, sc_tr), batch_size, True, pin_memory=False, num_workers=0),\n            \"validation\": DataLoader(ClimateDatasetWithScenario(X_val, y_val, sc_val), batch_size, False, pin_memory=False, num_workers=0),\n            \"test\": DataLoader(ClimateDatasetWithScenario(X_te, y_te, sc_te), batch_size, False, pin_memory=False, num_workers=0)\n        }\n        \n        logger.info(f\"   üì¶ Train:{len(X_tr)} Val:{len(X_val)} Test:{len(X_te)}\")\n        return splits, loaders\n\n\nprint(\"‚úÖ Fixed DataLoader loaded\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-09T09:58:13.711345Z","iopub.execute_input":"2025-10-09T09:58:13.711806Z","iopub.status.idle":"2025-10-09T09:58:13.748074Z","shell.execute_reply.started":"2025-10-09T09:58:13.711789Z","shell.execute_reply":"2025-10-09T09:58:13.747361Z"}},"outputs":[{"name":"stdout","text":"‚úÖ Fixed DataLoader loaded\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"## üß† Section 4: ClimAx Model (Memory-Efficient)","metadata":{}},{"cell_type":"code","source":"class MemoryEfficientPatchEmbedding(nn.Module):\n    \"\"\"Memory-efficient patch embedding with conv2d.\"\"\"\n    \n    def __init__(self, patch_size: int, in_channels: int, embed_dim: int, \n                 spatial_h: int, spatial_w: int):\n        super().__init__()\n        self.patch_size = patch_size\n        self.in_channels = in_channels\n        self.embed_dim = embed_dim\n        self.spatial_h = spatial_h\n        self.spatial_w = spatial_w\n        \n        # Projection layer\n        self.proj = nn.Conv2d(in_channels, embed_dim, \n                             kernel_size=patch_size, stride=patch_size)\n        \n        # Calculate number of patches\n        self.num_patches_h = spatial_h // patch_size\n        self.num_patches_w = spatial_w // patch_size\n        self.num_patches = self.num_patches_h * self.num_patches_w\n        \n        logger.info(f\"üîß PatchEmbed: {spatial_h}√ó{spatial_w} ‚Üí {self.num_patches_h}√ó{self.num_patches_w} = {self.num_patches} patches\")\n        \n    def forward(self, x):\n        # x: (B, T, C, H, W)\n        B, T, C, H, W = x.shape\n        \n        # Verify dimensions\n        if H != self.spatial_h or W != self.spatial_w:\n            raise ValueError(f\"Input spatial dims ({H}, {W}) don't match expected ({self.spatial_h}, {self.spatial_w})\")\n        \n        # Process in chunks to save memory\n        x = x.reshape(B * T, C, H, W)\n        x = self.proj(x)  # (B*T, embed_dim, H', W')\n        x = x.flatten(2).transpose(1, 2)  # (B*T, num_patches, embed_dim)\n        x = x.reshape(B, T, self.num_patches, self.embed_dim)\n        \n        return x\n\n\nclass MemoryEfficientPositionalEmbedding(nn.Module):\n    \"\"\"Memory-efficient positional embeddings.\"\"\"\n    \n    def __init__(self, num_patches: int, num_timesteps: int, embed_dim: int):\n        super().__init__()\n        self.num_patches = num_patches\n        self.num_timesteps = num_timesteps\n        self.embed_dim = embed_dim\n        \n        # Use smaller embeddings\n        self.spatial_pos_embed = nn.Parameter(torch.zeros(1, 1, num_patches, embed_dim))\n        self.temporal_pos_embed = nn.Parameter(torch.zeros(1, num_timesteps, 1, embed_dim))\n        \n        # Initialize with smaller std\n        nn.init.trunc_normal_(self.spatial_pos_embed, std=0.01)\n        nn.init.trunc_normal_(self.temporal_pos_embed, std=0.01)\n    \n    def forward(self, x):\n        B, T, P, D = x.shape\n        \n        if P != self.num_patches:\n            raise ValueError(f\"Number of patches mismatch: got {P}, expected {self.num_patches}\")\n        if T > self.num_timesteps:\n            raise ValueError(f\"Timesteps exceed maximum: got {T}, max {self.num_timesteps}\")\n        \n        return x + self.spatial_pos_embed + self.temporal_pos_embed[:, :T, :, :]\n\n\nclass MemoryEfficientTransformerBlock(nn.Module):\n    \"\"\"Memory-efficient Transformer block with gradient checkpointing.\"\"\"\n    \n    def __init__(self, embed_dim: int, num_heads: int, mlp_ratio: float = 4.0,\n                 dropout: float = 0.1, attention_dropout: float = 0.1):\n        super().__init__()\n        \n        self.norm1 = nn.LayerNorm(embed_dim)\n        self.attn = nn.MultiheadAttention(\n            embed_dim, num_heads, \n            dropout=attention_dropout, \n            batch_first=True\n        )\n        \n        self.norm2 = nn.LayerNorm(embed_dim)\n        mlp_hidden_dim = int(embed_dim * mlp_ratio)\n        self.mlp = nn.Sequential(\n            nn.Linear(embed_dim, mlp_hidden_dim),\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.Linear(mlp_hidden_dim, embed_dim),\n            nn.Dropout(dropout)\n        )\n    \n    def forward(self, x):\n        # Self-attention with residual\n        normed = self.norm1(x)\n        attn_out, _ = self.attn(normed, normed, normed)\n        x = x + attn_out\n        \n        # FFN with residual\n        x = x + self.mlp(self.norm2(x))\n        \n        return x\n\n\nclass ClimAxMemoryEfficientModel(nn.Module):\n    \"\"\"\n    ClimAx Model - MEMORY EFFICIENT VERSION\n    \n    ‚úÖ Designed for 6GB GPU\n    ‚úÖ Uses ConvLSTM spatial dimensions (9√ó19)\n    ‚úÖ Gradient checkpointing enabled\n    ‚úÖ Optimized attention mechanisms\n    ‚úÖ Supports NUM_INPUT_CHANNELS for scenario embeddings\n    \"\"\"\n    \n    def __init__(self, config, spatial_h: Optional[int] = None, \n                 spatial_w: Optional[int] = None, use_checkpointing: bool = True):\n        super().__init__()\n        \n        # Get spatial dims - prioritize 9√ó19 for memory efficiency\n        if spatial_h is not None and spatial_w is not None:\n            self.spatial_h = spatial_h\n            self.spatial_w = spatial_w\n        else:\n            # Default to ConvLSTM's memory-efficient dimensions\n            self.spatial_h = self._get_config_attr(config, 'SPATIAL_HEIGHT', 9)\n            self.spatial_w = self._get_config_attr(config, 'SPATIAL_WIDTH', 19)\n        \n        # CRITICAL: Warn if dimensions are too large\n        total_spatial = self.spatial_h * self.spatial_w\n        if total_spatial > 500:\n            logger.warning(f\"‚ö†Ô∏è  Large spatial dimensions ({self.spatial_h}√ó{self.spatial_w} = {total_spatial}) may cause OOM!\")\n            logger.warning(f\"   Consider downsampling to 9√ó19 (171 pixels) like ConvLSTM\")\n        \n        # Get model params\n        self.patch_size = self._get_config_attr(config, 'PATCH_SIZE', 2)\n        self.embed_dim = self._get_config_attr(config, 'EMBED_DIM', 128)\n        self.depth = self._get_config_attr(config, 'DEPTH', 8)\n        self.num_heads = self._get_config_attr(config, 'NUM_HEADS', 8)\n        self.mlp_ratio = self._get_config_attr(config, 'MLP_RATIO', 4.0)\n        self.dropout_rate = self._get_config_attr(config, 'DROPOUT_RATE', 0.1)\n        self.attention_dropout = self._get_config_attr(config, 'ATTENTION_DROPOUT', 0.1)\n        \n        # Reduce model size if needed\n        if total_spatial > 300:\n            logger.warning(\"   ‚Üí Reducing embed_dim from 128 to 64 to save memory\")\n            self.embed_dim = 64\n            logger.warning(\"   ‚Üí Reducing depth from 8 to 4 to save memory\")\n            self.depth = 4\n        \n        # ‚úÖ FIXED: Input channels - prioritize NUM_INPUT_CHANNELS\n        self.num_input_vars = self._get_config_attr(config, 'NUM_INPUT_CHANNELS', None)\n        if self.num_input_vars is None:\n            # Fall back to counting INPUT_VARIABLES\n            input_vars = self._get_config_attr(config, 'INPUT_VARIABLES', [])\n            self.num_input_vars = len(input_vars) if input_vars else 4\n            logger.info(f\"   üìä Input channels from INPUT_VARIABLES: {self.num_input_vars}\")\n        else:\n            logger.info(f\"   üìä Input channels from NUM_INPUT_CHANNELS: {self.num_input_vars}\")\n        \n        self.seq_in_len = self._get_config_attr(config, 'SEQUENCE_INPUT_LENGTH', 12)\n        self.seq_out_len = self._get_config_attr(config, 'SEQUENCE_OUTPUT_LENGTH', 3)\n        \n        # Calculate patches\n        self.num_patches_h = self.spatial_h // self.patch_size\n        self.num_patches_w = self.spatial_w // self.patch_size\n        self.num_patches = self.num_patches_h * self.num_patches_w\n        \n        # Memory estimation\n        tokens_per_batch = self.num_patches * self.seq_in_len\n        attn_memory_gb = (tokens_per_batch ** 2 * 4) / (1024 ** 3)  # float32 bytes to GB\n        \n        logger.info(f\"üîß Memory-Efficient ClimAx initialized:\")\n        logger.info(f\"   Spatial: {self.spatial_h}√ó{self.spatial_w} ‚Üí {self.num_patches} patches\")\n        logger.info(f\"   Tokens per sample: {tokens_per_batch}\")\n        logger.info(f\"   Est. attention memory: {attn_memory_gb:.2f} GB per batch\")\n        logger.info(f\"   Embed dim: {self.embed_dim}, Depth: {self.depth}\")\n        logger.info(f\"   Gradient checkpointing: {use_checkpointing}\")\n        \n        if attn_memory_gb > 4:\n            logger.error(f\"‚ùå Estimated memory ({attn_memory_gb:.2f} GB) exceeds safe limit!\")\n            logger.error(f\"   Please reduce spatial dimensions or batch size\")\n            raise RuntimeError(f\"Model too large for 6GB GPU! Need ~{attn_memory_gb:.1f}GB\")\n        \n        self.use_checkpointing = use_checkpointing\n        \n        # Patch embedding\n        self.patch_embed = MemoryEfficientPatchEmbedding(\n            patch_size=self.patch_size,\n            in_channels=self.num_input_vars,\n            embed_dim=self.embed_dim,\n            spatial_h=self.spatial_h,\n            spatial_w=self.spatial_w\n        )\n        \n        # Positional embeddings\n        self.pos_embed = MemoryEfficientPositionalEmbedding(\n            num_patches=self.num_patches,\n            num_timesteps=self.seq_in_len,\n            embed_dim=self.embed_dim\n        )\n        \n        # Transformer blocks\n        self.blocks = nn.ModuleList([\n            MemoryEfficientTransformerBlock(\n                embed_dim=self.embed_dim,\n                num_heads=self.num_heads,\n                mlp_ratio=self.mlp_ratio,\n                dropout=self.dropout_rate,\n                attention_dropout=self.attention_dropout\n            ) for _ in range(self.depth)\n        ])\n        \n        self.norm = nn.LayerNorm(self.embed_dim)\n        \n        # Lightweight prediction head\n        self.head = nn.Sequential(\n            nn.Linear(self.embed_dim, self.embed_dim),\n            nn.GELU(),\n            nn.Dropout(self.dropout_rate),\n            nn.Linear(self.embed_dim, self.patch_size * self.patch_size * self.seq_out_len)\n        )\n        \n        # Count parameters\n        self.num_params = sum(p.numel() for p in self.parameters())\n        logger.info(f\"‚úÖ Model ready - Parameters: {self.num_params:,}\")\n    \n    def _get_config_attr(self, config, attr_name, default):\n        \"\"\"Safely get attribute from config.\"\"\"\n        if hasattr(config, attr_name):\n            return getattr(config, attr_name)\n        \n        for ns in ['data', 'model', 'training']:\n            if hasattr(config, ns):\n                ns_obj = getattr(config, ns)\n                if ns_obj is not None and hasattr(ns_obj, attr_name):\n                    return getattr(ns_obj, attr_name)\n        \n        return default\n    \n    def forward(self, x):\n        \"\"\"\n        Memory-efficient forward pass with gradient checkpointing.\n        \n        Args:\n            x: (B, T, C, H, W) input tensor\n        \n        Returns:\n            (B, T_out, H, W) predictions\n        \"\"\"\n        B, T, C, H, W = x.shape\n        \n        # Store original spatial dims for reconstruction\n        original_h, original_w = H, W\n        \n        # Verify dimensions match\n        if H != self.spatial_h or W != self.spatial_w:\n            raise ValueError(\n                f\"Input spatial dims ({H}, {W}) don't match model ({self.spatial_h}, {self.spatial_w}). \"\n                f\"Please downsample your data to {self.spatial_h}√ó{self.spatial_w} before training.\"\n            )\n        \n        # Patch embedding\n        x = self.patch_embed(x)  # (B, T, num_patches, embed_dim)\n        \n        # Add positional embeddings\n        x = self.pos_embed(x)\n        \n        # Flatten for transformer\n        B, T, P, D = x.shape\n        x = x.reshape(B, T * P, D)\n        \n        # Apply transformer blocks with gradient checkpointing\n        for i, block in enumerate(self.blocks):\n            if self.use_checkpointing and self.training:\n                x = checkpoint(block, x, use_reentrant=False)\n            else:\n                x = block(x)\n        \n        x = self.norm(x)\n        \n        # Prediction head\n        x = self.head(x)  # (B, T*P, patch_size^2 * seq_out_len)\n        \n        # Reshape to spatial output\n        x = x.reshape(B, T, P, self.patch_size, self.patch_size, self.seq_out_len)\n        \n        # Reorganize patches back to spatial grid\n        x = x.reshape(B, T, self.num_patches_h, self.num_patches_w, \n                     self.patch_size, self.patch_size, self.seq_out_len)\n        \n        # Merge patches: (B, T, seq_out_len, H_recon, W_recon)\n        x = x.permute(0, 1, 6, 2, 4, 3, 5).contiguous()\n        \n        # Calculate reconstructed dimensions\n        h_recon = self.num_patches_h * self.patch_size\n        w_recon = self.num_patches_w * self.patch_size\n        \n        x = x.reshape(B, T, self.seq_out_len, h_recon, w_recon)\n        \n        # Upsample if needed to match original dimensions\n        if h_recon != original_h or w_recon != original_w:\n            x = x.reshape(B * T * self.seq_out_len, 1, h_recon, w_recon)\n            x = F.interpolate(x, size=(original_h, original_w), mode='bilinear', align_corners=False)\n            x = x.reshape(B, T, self.seq_out_len, original_h, original_w)\n        \n        # Average over input timesteps\n        x = x.mean(dim=1)  # (B, seq_out_len, H, W)\n        \n        return x\n\n\nprint(\"‚úÖ ClimAxMemoryEfficientModel class defined\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-09T09:58:13.748725Z","iopub.execute_input":"2025-10-09T09:58:13.748909Z","iopub.status.idle":"2025-10-09T09:58:13.776215Z","shell.execute_reply.started":"2025-10-09T09:58:13.748895Z","shell.execute_reply":"2025-10-09T09:58:13.775512Z"}},"outputs":[{"name":"stdout","text":"‚úÖ ClimAxMemoryEfficientModel class defined\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"## üöÇ Section 5: Training Function","metadata":{}},{"cell_type":"code","source":"# =============================================================================\n# üöÇ SECTION 5: TRAINING FUNCTION - FIXED WITH DEBUG LOGGING\n# =============================================================================\n\ndef train_single_institution_multi_scenario(institution: str, config: Phase4Config) -> dict:\n    \"\"\"Train a single institution - FIXED with extensive debug logging\"\"\"\n    \n    print(f\"\\n{'='*80}\")\n    print(f\"üåç Training: {institution}\")\n    print(f\"{'='*80}\")\n    \n    start_time = time.time()\n    \n    # Check for existing model\n    model_path = os.path.join(OUTPUT_DIR, \"checkpoints\", f\"{institution}_multiscenario_best.pt\")\n    \n    if RESUME_IF_MODEL_EXISTS and os.path.exists(model_path):\n        print(f\"‚≠ê Model exists - skipping {institution}\")\n        return {'success': True, 'institution': institution, 'skipped': True}\n    \n    try:\n        # STEP 1: Create data loader\n        print(\"üìä Step 1/8: Initializing data loader...\")\n        sys.stdout.flush()\n        dl = Phase4MultiScenarioDataLoader(config, target_h=TARGET_SPATIAL_H, target_w=TARGET_SPATIAL_W)\n        print(\"‚úÖ Data loader created\")\n        sys.stdout.flush()\n        \n        # STEP 2: Discover files\n        print(\"üîç Step 2/8: Discovering files...\")\n        sys.stdout.flush()\n        all_scenario_files = dl.discover_files_multi_scenario(institution)\n        \n        has_data = any(\n            bool(files['inputs']) or bool(files['outputs'])\n            for files in all_scenario_files.values()\n        )\n        \n        if not has_data:\n            print(f\"‚ö†Ô∏è No data for {institution}\")\n            return {'success': False, 'institution': institution, 'error': 'No data'}\n        \n        print(\"‚úÖ File discovery complete\")\n        sys.stdout.flush()\n        \n        # STEP 3: Load all scenarios\n        print(\"üìä Step 3/8: Loading data...\")\n        sys.stdout.flush()\n        all_scenario_data = dl.load_all_scenarios(all_scenario_files)\n        \n        if not all_scenario_data:\n            print(f\"‚ö†Ô∏è Failed to load data\")\n            return {'success': False, 'institution': institution, 'error': 'Load failed'}\n        \n        print(\"‚úÖ Data loaded\")\n        sys.stdout.flush()\n        \n        # STEP 4: Create sequences\n        print(\"üîÑ Step 4/8: Creating sequences...\")\n        sys.stdout.flush()\n        data_splits, dataloaders = dl.create_multi_scenario_sequences(all_scenario_data)\n        print(\"‚úÖ Sequences created\")\n        sys.stdout.flush()\n        \n        # STEP 5: Create model\n        print(\"üß† Step 5/8: Creating model...\")\n        sys.stdout.flush()\n        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n        print(f\"   Using device: {device}\")\n        \n        model = ClimAxMemoryEfficientModel(\n            config,\n            spatial_h=TARGET_SPATIAL_H,\n            spatial_w=TARGET_SPATIAL_W,\n            use_checkpointing=USE_CHECKPOINTING\n        ).to(device)\n        \n        print(\"‚úÖ Model created\")\n        sys.stdout.flush()\n        \n        # STEP 6: Setup training\n        print(\"‚öôÔ∏è Step 6/8: Setting up optimizer...\")\n        sys.stdout.flush()\n        optimizer = optim.AdamW(\n            model.parameters(),\n            lr=config.LEARNING_RATE,\n            weight_decay=config.WEIGHT_DECAY\n        )\n        criterion = nn.MSELoss()\n        \n        history = {\n            'train_loss': [],\n            'val_loss': [],\n            'scenario_losses': {}\n        }\n        best_val_loss = float('inf')\n        \n        print(\"‚úÖ Optimizer ready\")\n        sys.stdout.flush()\n        \n        print(f\"\\nüöÇ Step 7/8: Training for {EPOCHS} epochs...\")\n        sys.stdout.flush()\n        \n        # STEP 7: Training loop with progress bars\n        epoch_pbar = tqdm(range(EPOCHS), desc=f\"üåç {institution}\", position=0, leave=True)\n        \n        for ep in epoch_pbar:\n            print(f\"\\n--- Epoch {ep+1}/{EPOCHS} ---\")\n            sys.stdout.flush()\n            \n            # === TRAINING PHASE ===\n            model.train()\n            train_loss = 0.0\n            train_batches = 0\n            \n            print(f\"Training: Processing {len(dataloaders['train'])} batches...\")\n            sys.stdout.flush()\n            \n            train_pbar = tqdm(\n                dataloaders['train'], \n                desc=f\"   Train Epoch {ep+1}/{EPOCHS}\",\n                position=1,\n                leave=False,\n                bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}]'\n            )\n            \n            for batch_idx, batch in enumerate(train_pbar):\n                try:\n                    X, y, scenarios = batch\n                    X = X.to(device)\n                    y = y.to(device)\n                    \n                    optimizer.zero_grad()\n                    preds = model(X)\n                    \n                    # Align shapes\n                    if preds.shape != y.shape:\n                        if preds.ndim == 4 and y.ndim == 5:\n                            preds = preds.unsqueeze(2)\n                    \n                    loss = criterion(preds, y)\n                    loss.backward()\n                    \n                    torch.nn.utils.clip_grad_norm_(model.parameters(), config.CLIP_GRAD_NORM)\n                    optimizer.step()\n                    \n                    train_loss += loss.item()\n                    train_batches += 1\n                    \n                    # Update progress bar\n                    train_pbar.set_postfix({'loss': f'{loss.item():.6f}'})\n                    \n                    # Debug print every 5 batches\n                    if batch_idx % 5 == 0:\n                        print(f\"      Batch {batch_idx}: loss={loss.item():.6f}\")\n                        sys.stdout.flush()\n                    \n                    if train_batches % 10 == 0:\n                        torch.cuda.empty_cache()\n                \n                except Exception as e:\n                    print(f\"\\n‚ùå Error in training batch {batch_idx}: {e}\")\n                    import traceback\n                    traceback.print_exc()\n                    raise\n            \n            avg_train_loss = train_loss / max(1, train_batches)\n            history['train_loss'].append(avg_train_loss)\n            \n            print(f\"   ‚úì Train loss: {avg_train_loss:.6f}\")\n            sys.stdout.flush()\n            \n            # === VALIDATION PHASE ===\n            model.eval()\n            val_loss = 0.0\n            val_batches = 0\n            scenario_losses = {s: [] for s in SCENARIOS.keys()}\n            \n            print(f\"Validation: Processing {len(dataloaders['validation'])} batches...\")\n            sys.stdout.flush()\n            \n            val_pbar = tqdm(\n                dataloaders['validation'],\n                desc=f\"   Val Epoch {ep+1}/{EPOCHS}\",\n                position=1,\n                leave=False,\n                bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt}'\n            )\n            \n            with torch.no_grad():\n                for batch_idx, batch in enumerate(val_pbar):\n                    try:\n                        Xv, yv, scenarios_v = batch\n                        Xv = Xv.to(device)\n                        yv = yv.to(device)\n                        \n                        preds = model(Xv)\n                        \n                        if preds.shape != yv.shape:\n                            if preds.ndim == 4 and yv.ndim == 5:\n                                preds = preds.unsqueeze(2)\n                        \n                        batch_loss = criterion(preds, yv)\n                        val_loss += batch_loss.item()\n                        val_batches += 1\n                        \n                        # Track per-scenario losses\n                        for i, scenario_id in enumerate(scenarios_v.cpu().numpy()):\n                            scenario_name = [k for k, v in SCENARIOS.items() if v == scenario_id][0]\n                            scenario_losses[scenario_name].append(batch_loss.item())\n                        \n                        val_pbar.set_postfix({'loss': f'{batch_loss.item():.6f}'})\n                    \n                    except Exception as e:\n                        print(f\"\\n‚ùå Error in validation batch {batch_idx}: {e}\")\n                        import traceback\n                        traceback.print_exc()\n                        raise\n            \n            avg_val_loss = val_loss / max(1, val_batches)\n            history['val_loss'].append(avg_val_loss)\n            \n            print(f\"   ‚úì Val loss: {avg_val_loss:.6f}\")\n            sys.stdout.flush()\n            \n            # Calculate per-scenario averages\n            avg_scenario_losses = {\n                s: np.mean(losses) if losses else 0.0\n                for s, losses in scenario_losses.items()\n            }\n            history['scenario_losses'][f'epoch_{ep+1}'] = avg_scenario_losses\n            \n            # Update best model\n            is_best = avg_val_loss < best_val_loss\n            if is_best:\n                best_val_loss = avg_val_loss\n                best_indicator = \"‚≠ê NEW BEST\"\n            else:\n                best_indicator = \"\"\n            \n            # Update main progress bar\n            epoch_pbar.set_postfix({\n                'train': f'{avg_train_loss:.6f}',\n                'val': f'{avg_val_loss:.6f}',\n                'best': f'{best_val_loss:.6f}',\n                'status': best_indicator\n            })\n            \n            # Memory cleanup\n            torch.cuda.empty_cache()\n            gc.collect()\n        \n        # Close progress bars\n        epoch_pbar.close()\n        \n        # STEP 8: Save model\n        print(\"üíæ Step 8/8: Saving model...\")\n        sys.stdout.flush()\n        torch.save({\n            'institution': institution,\n            'model_state_dict': model.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            'history': history,\n            'best_val_loss': best_val_loss,\n            'spatial_dims': [TARGET_SPATIAL_H, TARGET_SPATIAL_W],\n            'scenarios': list(SCENARIOS.keys()),\n            'timestamp': datetime.now().isoformat()\n        }, model_path)\n        \n        print(f\"‚úÖ Saved to {model_path}\")\n        sys.stdout.flush()\n        \n        # Save results\n        summary = {\n            'institution': institution,\n            'training_time': time.time() - start_time,\n            'epochs_trained': len(history['train_loss']),\n            'best_val_loss': best_val_loss,\n            'spatial_dims': [TARGET_SPATIAL_H, TARGET_SPATIAL_W],\n            'scenarios': list(SCENARIOS.keys()),\n            'final_scenario_losses': history['scenario_losses'].get(f'epoch_{EPOCHS}', {}),\n            'timestamp': datetime.now().isoformat()\n        }\n        \n        results_path = os.path.join(OUTPUT_DIR, \"logs\", f\"{institution}_phase4_results.json\")\n        with open(results_path, 'w') as f:\n            json.dump(summary, f, indent=2)\n        \n        return {'success': True, 'institution': institution, 'results': summary}\n    \n    except Exception as e:\n        print(f\"\\n‚ùå FATAL ERROR: {e}\")\n        import traceback\n        traceback.print_exc()\n        sys.stdout.flush()\n        return {'success': False, 'institution': institution, 'error': str(e)}\n    \n    finally:\n        torch.cuda.empty_cache()\n        gc.collect()\n\n\nprint(\"‚úÖ Fixed training function loaded\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-09T09:58:13.776918Z","iopub.execute_input":"2025-10-09T09:58:13.777137Z","iopub.status.idle":"2025-10-09T09:58:13.802269Z","shell.execute_reply.started":"2025-10-09T09:58:13.777113Z","shell.execute_reply":"2025-10-09T09:58:13.801587Z"}},"outputs":[{"name":"stdout","text":"‚úÖ Fixed training function loaded\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"## üöÄ Section 6: Main Training Loop","metadata":{}},{"cell_type":"code","source":"# =============================================================================\n# üöÄ SECTION 6: MAIN TRAINING EXECUTION - FIXED WITH DEBUG\n# =============================================================================\n\nprint(\"=\"*80)\nprint(\"üåç ClimAx Phase 4 - Multi-Scenario Training\")\nprint(\"=\"*80)\nprint(f\"Start time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\nsys.stdout.flush()\n\n# Create config\nconfig = Phase4Config()\n\n# Determine institutions to train\nif SMOKE_TEST:\n    institutions = ALL_INSTITUTIONS[:2]\n    print(f\"‚ö†Ô∏è SMOKE TEST - Training only {len(institutions)} institutions\")\nelse:\n    institutions = ALL_INSTITUTIONS\n    print(f\"üìã Training all {len(institutions)} institutions\")\n\nprint(f\"   Scenarios: {list(SCENARIOS.keys())}\")\nprint(f\"   Spatial: {TARGET_SPATIAL_H}√ó{TARGET_SPATIAL_W}\")\nprint(f\"   Batch size: {BATCH_SIZE}\")\nprint(f\"   Epochs: {EPOCHS}\")\nprint(f\"   Patch size: {config.PATCH_SIZE}\")\nprint(f\"   Input channels: {config.NUM_INPUT_CHANNELS}\")\nsys.stdout.flush()\n\n# GPU Check\nif torch.cuda.is_available():\n    print(f\"\\nüéÆ GPU Status:\")\n    print(f\"   Device: {torch.cuda.get_device_name(0)}\")\n    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / (1024**3):.1f} GB\")\n    print(f\"   Current allocation: {torch.cuda.memory_allocated() / (1024**2):.1f} MB\")\nelse:\n    print(\"\\n‚ö†Ô∏è WARNING: No GPU available - will be VERY slow!\")\nsys.stdout.flush()\n\nif SKIP_TRAINING:\n    print(\"SKIP_TRAINING=True - Loading existing results...\")\n    summary_path = os.path.join(OUTPUT_DIR, \"logs\", \"phase4_training_summary.json\")\n    if os.path.exists(summary_path):\n        with open(summary_path, 'r') as f:\n            summary = json.load(f)\n        print(\"‚úÖ Loaded existing results\")\n    else:\n        print(\"‚ö†Ô∏è No existing results found\")\n        summary = {}\nelse:\n    # Training loop with main progress bar\n    print(\"\\n\" + \"=\"*80)\n    print(\"üöÇ STARTING TRAINING\")\n    print(\"=\"*80)\n    sys.stdout.flush()\n    \n    t0 = time.time()\n    all_results = []\n    \n    main_pbar = tqdm(\n        institutions,\n        desc=\"üåç Overall Progress\",\n        position=0,\n        bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}]'\n    )\n    \n    for inst_idx, institution in enumerate(main_pbar):\n        print(f\"\\n{'='*80}\")\n        print(f\"Institution {inst_idx+1}/{len(institutions)}: {institution}\")\n        print(f\"{'='*80}\")\n        sys.stdout.flush()\n        \n        main_pbar.set_description(f\"üåç Training {institution}\")\n        \n        try:\n            result = train_single_institution_multi_scenario(institution, config)\n            all_results.append(result)\n            \n            # Update main progress with status\n            successful = sum(1 for r in all_results if r.get('success') and not r.get('skipped'))\n            skipped = sum(1 for r in all_results if r.get('skipped'))\n            failed = sum(1 for r in all_results if not r.get('success'))\n            \n            print(f\"\\nüìä Progress Summary:\")\n            print(f\"   ‚úÖ Successful: {successful}\")\n            print(f\"   ‚≠ê Skipped: {skipped}\")\n            print(f\"   ‚ùå Failed: {failed}\")\n            sys.stdout.flush()\n            \n            main_pbar.set_postfix({\n                'success': successful,\n                'skipped': skipped,\n                'failed': failed\n            })\n            \n            # Save progress after each institution\n            progress = {\n                'completed': len(all_results),\n                'total': len(institutions),\n                'results': all_results,\n                'timestamp': datetime.now().isoformat()\n            }\n            \n            progress_path = os.path.join(OUTPUT_DIR, \"logs\", \"phase4_progress.json\")\n            with open(progress_path, 'w') as f:\n                json.dump(progress, f, indent=2)\n            print(f\"üíæ Progress saved to {progress_path}\")\n            sys.stdout.flush()\n        \n        except Exception as e:\n            print(f\"\\n‚ùå CRITICAL ERROR processing {institution}: {e}\")\n            import traceback\n            traceback.print_exc()\n            sys.stdout.flush()\n            \n            all_results.append({\n                'success': False,\n                'institution': institution,\n                'error': str(e)\n            })\n    \n    main_pbar.close()\n    elapsed = time.time() - t0\n    \n    # Compile summary\n    successful = [r for r in all_results if r.get('success') and not r.get('skipped')]\n    skipped = [r for r in all_results if r.get('skipped')]\n    failed = [r for r in all_results if not r.get('success')]\n    \n    summary = {\n        'phase': 'phase4_multi_scenario',\n        'total_institutions': len(institutions),\n        'successful': len(successful),\n        'skipped': len(skipped),\n        'failed': len(failed),\n        'total_time_hours': elapsed / 3600,\n        'scenarios': list(SCENARIOS.keys()),\n        'results': all_results,\n        'timestamp': datetime.now().isoformat()\n    }\n    \n    # Save summary\n    summary_path = os.path.join(OUTPUT_DIR, \"logs\", \"phase4_training_summary.json\")\n    with open(summary_path, 'w') as f:\n        json.dump(summary, f, indent=2)\n    \n    print(f\"\\n{'='*80}\")\n    print(\"‚úÖ Phase 4 Training Complete!\")\n    print(f\"{'='*80}\")\n    print(f\"Total institutions: {len(institutions)}\")\n    print(f\"  ‚úÖ Successful: {len(successful)}\")\n    print(f\"  ‚≠ê Skipped: {len(skipped)}\")\n    print(f\"  ‚ùå Failed: {len(failed)}\")\n    print(f\"Total time: {elapsed/3600:.2f} hours\")\n    print(f\"{'='*80}\")\n    sys.stdout.flush()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-09T09:58:13.803095Z","iopub.execute_input":"2025-10-09T09:58:13.803396Z"}},"outputs":[{"name":"stdout","text":"================================================================================\nüåç ClimAx Phase 4 - Multi-Scenario Training\n================================================================================\nStart time: 2025-10-09 09:58:13\n‚ö†Ô∏è SMOKE TEST - Training only 2 institutions\n   Scenarios: ['historical', 'ssp126', 'ssp245', 'ssp370', 'ssp585']\n   Spatial: 9√ó19\n   Batch size: 1\n   Epochs: 2\n   Patch size: 1\n   Input channels: 8\n\nüéÆ GPU Status:\n   Device: Tesla P100-PCIE-16GB\n   Memory: 15.9 GB\n   Current allocation: 0.0 MB\n\n================================================================================\nüöÇ STARTING TRAINING\n================================================================================\n","output_type":"stream"},{"name":"stderr","text":"üåç Overall Progress:   0%|          | 0/2 [00:00<?]","output_type":"stream"},{"name":"stdout","text":"\n================================================================================\nInstitution 1/2: AWI-CM-1-1-MR\n================================================================================\n","output_type":"stream"},{"name":"stderr","text":"üåç Training AWI-CM-1-1-MR:   0%|          | 0/2 [00:00<?]","output_type":"stream"},{"name":"stdout","text":"\n================================================================================\nüåç Training: AWI-CM-1-1-MR\n================================================================================\nüìä Step 1/8: Initializing data loader...\n‚úÖ Data loader created\nüîç Step 2/8: Discovering files...\n","output_type":"stream"}],"execution_count":null},{"cell_type":"markdown","source":"## üìä Section 7: Display Results","metadata":{}},{"cell_type":"code","source":"# Display results\nif summary:\n    print(\"\\n\" + \"=\"*80)\n    print(\"üìä PHASE 4 RESULTS SUMMARY\")\n    print(\"=\"*80)\n    print(f\"Total institutions: {summary.get('total_institutions', 0)}\")\n    print(f\"  ‚úÖ Successful: {summary.get('successful', 0)}\")\n    print(f\"  ‚≠ê Skipped: {summary.get('skipped', 0)}\")\n    print(f\"  ‚ùå Failed: {summary.get('failed', 0)}\")\n    print(f\"Total time: {summary.get('total_time_hours', 0):.2f}h\")\n    print(f\"Scenarios: {summary.get('scenarios', [])}\")\n    \n    # Scenario performance\n    print(f\"\\nüåç Scenario Performance Across All Institutions:\")\n    print(\"=\"*80)\n    \n    scenario_perf = {s: [] for s in SCENARIOS.keys()}\n    \n    for result in summary.get('results', []):\n        if result.get('success') and not result.get('skipped'):\n            inst_results = result.get('results', {})\n            final_losses = inst_results.get('final_scenario_losses', {})\n            \n            for scenario, loss in final_losses.items():\n                if scenario in scenario_perf:\n                    scenario_perf[scenario].append(loss)\n    \n    for scenario, losses in scenario_perf.items():\n        if losses:\n            avg = np.mean(losses)\n            std = np.std(losses)\n            min_loss = np.min(losses)\n            max_loss = np.max(losses)\n            print(f\"   {scenario.upper():12s}: Avg={avg:.6f}¬±{std:.6f} | Min={min_loss:.6f} | Max={max_loss:.6f}\")\n        else:\n            print(f\"   {scenario.upper():12s}: No data\")\n    \n    print(\"=\"*80)\n\nprint(f\"\\n‚úÖ Results saved to: {OUTPUT_DIR}\")\nprint(\"=\"*80)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## üéâ Done!\n\nYour models are now trained and saved in the `climax_phase4_results/` directory.\n\n### üìÅ Output Files:\n- **Models**: `climax_phase4_results/checkpoints/*.pt`\n- **Logs**: `climax_phase4_results/logs/*.json`\n- **Summary**: `climax_phase4_results/logs/phase4_training_summary.json`\n\n### üîÑ Next Steps:\n1. Load trained models for inference\n2. Evaluate on test set\n3. Generate predictions for different scenarios\n4. Visualize results","metadata":{}}]}