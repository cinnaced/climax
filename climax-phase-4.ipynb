{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":13299687,"sourceType":"datasetVersion","datasetId":8429777},{"sourceId":13303078,"sourceType":"datasetVersion","datasetId":8432230}],"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 🌍 ClimAx Phase 4 - Complete Multi-Scenario Training\n\n**Complete notebook for training ClimAx models across multiple climate scenarios**\n\n---\n\n## 📋 Features\n- ✅ Multi-scenario training (historical + SSP126/245/370/585)\n- ✅ Stratified splitting (all scenarios in train/val/test)\n- ✅ Memory-efficient for 6GB GPU\n- ✅ Progress bars with tqdm\n- ✅ Automatic checkpointing\n\n## 🎯 Requirements\n- PyTorch with CUDA\n- 6GB+ GPU memory\n- Climate data in E:/Datasets/ (or update paths)\n\n---","metadata":{}},{"cell_type":"markdown","source":"## 📦 Section 1: Install & Import Dependencies","metadata":{}},{"cell_type":"code","source":"# =============================================================================\n# 📦 SECTION 1: INSTALL & IMPORT DEPENDENCIES - FIXED\n# =============================================================================\n\n# Install required packages (uncomment if needed on Kaggle)\n# !pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n# !pip install xarray netcdf4 scipy scikit-learn tqdm matplotlib\n\nimport os\nimport sys  # ✅ CRITICAL: Added for sys.stdout.flush()\nimport time\nimport json\nimport logging\nimport gc\nimport glob\nimport math\nfrom datetime import datetime\nfrom typing import Dict, List, Tuple, Optional, Any\n\nimport numpy as np\nimport xarray as xr\nfrom scipy.ndimage import zoom\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.utils.checkpoint import checkpoint\n\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Setup logging with more detail\nlogging.basicConfig(\n    level=logging.INFO,\n    format=\"%(asctime)s [%(levelname)s] %(message)s\"\n)\nlogger = logging.getLogger(\"phase4_training\")\n\nprint(\"✅ All imports successful!\")\nprint(f\"🎮 PyTorch version: {torch.__version__}\")\nprint(f\"🎮 CUDA available: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"🎮 GPU: {torch.cuda.get_device_name(0)}\")\n    print(f\"🎮 GPU Memory: {torch.cuda.get_device_properties(0).total_memory / (1024**3):.1f} GB\")\nelse:\n    print(\"⚠️ WARNING: No GPU detected - training will be VERY slow!\")\n\n# Force flush output immediately\nsys.stdout.flush()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-09T09:57:58.395218Z","iopub.execute_input":"2025-10-09T09:57:58.395824Z","iopub.status.idle":"2025-10-09T09:58:04.057368Z","shell.execute_reply.started":"2025-10-09T09:57:58.395799Z","shell.execute_reply":"2025-10-09T09:58:04.056583Z"}},"outputs":[{"name":"stdout","text":"✅ All imports successful!\n🎮 PyTorch version: 2.6.0+cu124\n🎮 CUDA available: True\n🎮 GPU: Tesla P100-PCIE-16GB\n🎮 GPU Memory: 15.9 GB\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"## ⚙️ Section 2: Configuration\n\n**⚠️ UPDATE THE DATA PATHS BELOW!**","metadata":{}},{"cell_type":"code","source":"# =============================================================================\n# 🌍 CONFIGURATION - KAGGLE PATHS\n# =============================================================================\n\n# ✅ Updated for Kaggle dataset structure\nINPUT_DATA_DIR = \"/kaggle/input/climate-dataset/Datasets/inputs/input4mips\"\nOUTPUT_DATA_DIR = \"/kaggle/input/climate-dataset/Datasets/outputs/CMIP6\"\nOUTPUT_DIR = \"climax_phase4_results\"\n\n# Training settings\nSMOKE_TEST = True              # True = 2 institutions, 2 epochs for testing\nRESUME_IF_MODEL_EXISTS = True  # Skip already trained models\nSKIP_TRAINING = False          # Set True to only load results\n\n# Memory settings\nTARGET_SPATIAL_H = 9\nTARGET_SPATIAL_W = 19\nBATCH_SIZE = 1\nUSE_CHECKPOINTING = True\nEPOCHS = 2 if SMOKE_TEST else 15  # Short for smoke test\n\n# Model settings\nEMBED_DIM = 64\nDEPTH = 4\nNUM_HEADS = 4\n\n# All 18 institutions\nALL_INSTITUTIONS = [\n    \"AWI-CM-1-1-MR\", \"BCC-CSM2-MR\", \"CAS-ESM2-0\", \"CESM2\",\n    \"CESM2-WACCM\", \"CMCC-CM2-SR5\", \"CMCC-ESM2\", \"CNRM-CM6-1-HR\",\n    \"EC-Earth3\", \"EC-Earth3-Veg\", \"EC-Earth3-Veg-LR\", \"FGOALS-f3-L\",\n    \"GFDL-ESM4\", \"INM-CM4-8\", \"INM-CM5-0\", \"MPI-ESM1-2-HR\",\n    \"MRI-ESM2-0\", \"TaiESM1\"\n]\n\nSCENARIOS = {\n    'historical': 0,\n    'ssp126': 1,\n    'ssp245': 2,\n    'ssp370': 3,\n    'ssp585': 4\n}\n\nprint(\"✅ Configuration loaded\")\nprint(f\"   📁 Input: {INPUT_DATA_DIR}\")\nprint(f\"   📁 Output: {OUTPUT_DATA_DIR}\")\nprint(f\"   💾 Results: /kaggle/working/{OUTPUT_DIR}\")\nprint(f\"   🗺️  Spatial: {TARGET_SPATIAL_H}×{TARGET_SPATIAL_W}\")\nprint(f\"   🏢 Institutions: {len(ALL_INSTITUTIONS)}\")\nprint(f\"   🌍 Scenarios: {list(SCENARIOS.keys())}\")\nprint(f\"   ⚙️  Smoke test: {SMOKE_TEST}\")\n\n# Verify paths exist\nif not os.path.exists(INPUT_DATA_DIR):\n    print(f\"\\n❌ ERROR: Input path not found!\")\n    print(f\"   Path: {INPUT_DATA_DIR}\")\n    print(f\"   Make sure 'climate-dataset' is attached to this notebook!\")\nelse:\n    print(f\"\\n✅ Input path verified\")\n\nif not os.path.exists(OUTPUT_DATA_DIR):\n    print(f\"\\n❌ ERROR: Output path not found!\")\n    print(f\"   Path: {OUTPUT_DATA_DIR}\")\n    print(f\"   Make sure 'climate-dataset' is attached to this notebook!\")\nelse:\n    print(f\"✅ Output path verified\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-09T09:58:04.058824Z","iopub.execute_input":"2025-10-09T09:58:04.059213Z","iopub.status.idle":"2025-10-09T09:58:04.080694Z","shell.execute_reply.started":"2025-10-09T09:58:04.059196Z","shell.execute_reply":"2025-10-09T09:58:04.079474Z"}},"outputs":[{"name":"stdout","text":"✅ Configuration loaded\n   📁 Input: /kaggle/input/climate-dataset/Datasets/inputs/input4mips\n   📁 Output: /kaggle/input/climate-dataset/Datasets/outputs/CMIP6\n   💾 Results: /kaggle/working/climax_phase4_results\n   🗺️  Spatial: 9×19\n   🏢 Institutions: 18\n   🌍 Scenarios: ['historical', 'ssp126', 'ssp245', 'ssp370', 'ssp585']\n   ⚙️  Smoke test: True\n\n✅ Input path verified\n✅ Output path verified\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# =============================================================================\n# 🔍 DEBUG: Inspect Data Structure\n# =============================================================================\n\nprint(\"=\"*80)\nprint(\"🔍 DEBUGGING: Checking actual file structure\")\nprint(\"=\"*80)\n\n# Check inputs structure\nprint(\"\\n📊 INPUT STRUCTURE:\")\ninput_base = \"/kaggle/input/climate-dataset/Datasets/inputs/input4mips\"\n\nif os.path.exists(input_base):\n    scenarios = os.listdir(input_base)\n    print(f\"✅ Found scenarios: {scenarios}\\n\")\n    \n    # Check historical scenario as example\n    hist_path = os.path.join(input_base, \"historical\")\n    if os.path.exists(hist_path):\n        print(f\"📁 Contents of historical/:\")\n        for item in os.listdir(hist_path):\n            item_path = os.path.join(hist_path, item)\n            if os.path.isdir(item_path):\n                # Count files in this directory\n                files = []\n                for root, dirs, filenames in os.walk(item_path):\n                    files.extend([f for f in filenames if f.endswith('.nc')])\n                print(f\"   📂 {item}/ → {len(files)} .nc files\")\n    else:\n        print(\"❌ historical/ not found\")\nelse:\n    print(f\"❌ Path not found: {input_base}\")\n\n# Check outputs structure\nprint(\"\\n🎯 OUTPUT STRUCTURE:\")\noutput_base = \"/kaggle/input/climate-dataset/Datasets/outputs/CMIP6\"\n\nif os.path.exists(output_base):\n    institutions = os.listdir(output_base)\n    print(f\"✅ Found {len(institutions)} institutions\\n\")\n    \n    # Check first institution as example\n    first_inst = institutions[0] if institutions else None\n    if first_inst:\n        inst_path = os.path.join(output_base, first_inst)\n        print(f\"📁 Contents of {first_inst}/:\")\n        \n        if os.path.isdir(inst_path):\n            items = os.listdir(inst_path)\n            print(f\"   Items: {items[:5]}...\")  # Show first 5\n            \n            # Check if scenarios are inside\n            for scenario in ['historical', 'ssp126', 'ssp245']:\n                scenario_path = os.path.join(inst_path, scenario)\n                if os.path.exists(scenario_path):\n                    print(f\"   ✓ {scenario}/ exists\")\n                    # Check for pr variable\n                    pr_path = os.path.join(scenario_path, \"pr\")\n                    if os.path.exists(pr_path):\n                        files = []\n                        for root, dirs, filenames in os.walk(pr_path):\n                            files.extend([f for f in filenames if f.endswith('.nc')])\n                        print(f\"      → pr/ → {len(files)} .nc files\")\n                else:\n                    print(f\"   ✗ {scenario}/ not found\")\nelse:\n    print(f\"❌ Path not found: {output_base}\")\n\nprint(\"\\n\" + \"=\"*80)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-09T09:58:04.081626Z","iopub.execute_input":"2025-10-09T09:58:04.081934Z","iopub.status.idle":"2025-10-09T09:58:13.700717Z","shell.execute_reply.started":"2025-10-09T09:58:04.081910Z","shell.execute_reply":"2025-10-09T09:58:13.699995Z"}},"outputs":[{"name":"stdout","text":"================================================================================\n🔍 DEBUGGING: Checking actual file structure\n================================================================================\n\n📊 INPUT STRUCTURE:\n✅ Found scenarios: ['ssp585', 'ssp370', 'historical', 'ssp126', 'ssp245']\n\n📁 Contents of historical/:\n   📂 CO2_sum/ → 165 .nc files\n   📂 SO2_sum/ → 495 .nc files\n   📂 BC_sum/ → 495 .nc files\n   📂 CH4_sum/ → 495 .nc files\n\n🎯 OUTPUT STRUCTURE:\n✅ Found 18 institutions\n\n📁 Contents of CMCC-CM2-SR5/:\n   Items: ['ssp585', 'ssp370', 'historical', 'ssp126', 'ssp245']...\n   ✓ historical/ exists\n      → pr/ → 165 .nc files\n   ✓ ssp126/ exists\n      → pr/ → 86 .nc files\n   ✓ ssp245/ exists\n      → pr/ → 86 .nc files\n\n================================================================================\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"class Phase4Config:\n    \"\"\"Minimal config for Phase 4 - FIXED for scenario channels\"\"\"\n    def __init__(self):\n        self.INPUT_DATA_DIR = INPUT_DATA_DIR\n        self.OUTPUT_DATA_DIR = OUTPUT_DATA_DIR\n        self.SPATIAL_HEIGHT = TARGET_SPATIAL_H\n        self.SPATIAL_WIDTH = TARGET_SPATIAL_W\n        self.SPATIAL_H = TARGET_SPATIAL_H\n        self.SPATIAL_W = TARGET_SPATIAL_W\n        self.PATCH_SIZE = 1\n        self.EMBED_DIM = EMBED_DIM\n        self.DEPTH = DEPTH\n        self.NUM_HEADS = NUM_HEADS\n        self.MLP_RATIO = 4.0\n        self.DROPOUT_RATE = 0.1\n        self.ATTENTION_DROPOUT = 0.1\n        self.DROP_PATH_RATE = 0.1\n        self.SEQUENCE_INPUT_LENGTH = 12\n        self.SEQUENCE_OUTPUT_LENGTH = 3\n        self.TEMPORAL_STRIDE = 1\n        self.BATCH_SIZE = BATCH_SIZE\n        self.TRAIN_RATIO = 0.7\n        self.VAL_RATIO = 0.15\n        self.LEARNING_RATE = 1e-4\n        self.WEIGHT_DECAY = 0.05\n        self.CLIP_GRAD_NORM = 1.0\n        \n        # Physical input variables (from files only)\n        self.INPUT_VARIABLES = [\n            'BC_anthro_fires', 'BC_no_fires',\n            'CH4_anthro_fires', 'CH4_no_fires',\n            'CO2_sum',\n            'SO2_anthro_fires', 'SO2_no_fires'\n        ]\n        \n        # ✅ FIXED: Total input channels (7 variables + 1 scenario channel)\n        self.NUM_INPUT_CHANNELS = 8\n        \n        self.OUTPUT_VARIABLE = 'pr'\n\n# Create directories\nos.makedirs(OUTPUT_DIR, exist_ok=True)\nos.makedirs(os.path.join(OUTPUT_DIR, \"checkpoints\"), exist_ok=True)\nos.makedirs(os.path.join(OUTPUT_DIR, \"logs\"), exist_ok=True)\nos.makedirs(os.path.join(OUTPUT_DIR, \"plots\"), exist_ok=True)\n\nprint(\"✅ Config class created\")\nprint(\"✅ Output directories created\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-09T09:58:13.701407Z","iopub.execute_input":"2025-10-09T09:58:13.701678Z","iopub.status.idle":"2025-10-09T09:58:13.709422Z","shell.execute_reply.started":"2025-10-09T09:58:13.701658Z","shell.execute_reply":"2025-10-09T09:58:13.708868Z"}},"outputs":[{"name":"stdout","text":"✅ Config class created\n✅ Output directories created\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"## 📊 Section 3: Data Loader (Stratified Multi-Scenario)","metadata":{}},{"cell_type":"code","source":"# =============================================================================\n# 📊 SECTION 3: DATA LOADER - FIXED VERSION (From working Python file)\n# =============================================================================\n\nclass ClimateDatasetWithScenario(Dataset):\n    \"\"\"PyTorch dataset with scenario information\"\"\"\n    \n    def __init__(self, X: np.ndarray, y: np.ndarray, scenarios: np.ndarray):\n        self.X = torch.from_numpy(X.astype(np.float32))\n        self.y = torch.from_numpy(y.astype(np.float32))\n        self.scenarios = torch.from_numpy(scenarios.astype(np.float32))\n    \n    def __len__(self):\n        return self.X.shape[0]\n    \n    def __getitem__(self, idx):\n        return self.X[idx], self.y[idx], self.scenarios[idx]\n\n\nclass Phase4MultiScenarioDataLoader:\n    \"\"\"\n    Multi-Scenario Data Loader - FIXED VERSION from working Python file\n    ✅ Robust file discovery\n    ✅ Better error handling\n    ✅ Memory-efficient\n    \"\"\"\n    \n    def __init__(self, config, target_h: int = 9, target_w: int = 19):\n        self.input_base = config.INPUT_DATA_DIR\n        self.output_base = config.OUTPUT_DATA_DIR\n        self.target_h = target_h\n        self.target_w = target_w\n        self.scalers = {}\n        \n        self.scenarios = {'historical': 0, 'ssp126': 1, 'ssp245': 2, 'ssp370': 3, 'ssp585': 4}\n        self.input_variables = config.INPUT_VARIABLES\n        self.variable_dirs = {\n            'BC_anthro_fires': 'BC_sum', 'BC_no_fires': 'BC_sum',\n            'CH4_anthro_fires': 'CH4_sum', 'CH4_no_fires': 'CH4_sum',\n            'CO2_sum': 'CO2_sum',\n            'SO2_anthro_fires': 'SO2_sum', 'SO2_no_fires': 'SO2_sum'\n        }\n        self.output_variable = config.OUTPUT_VARIABLE\n        logger.info(f\"📊 DataLoader initialized: {target_h}×{target_w}\")\n    \n    def _matches_variable_pattern(self, filename: str, variable: str) -> bool:\n        \"\"\"Pattern matching for filenames\"\"\"\n        f = filename.lower()\n        patterns = {\n            'BC_anthro_fires': (['bc', 'anthro'], ['fire']),\n            'BC_no_fires': (['bc', 'no'], ['fire']),\n            'CH4_anthro_fires': (['ch4', 'anthro'], ['fire']),\n            'CH4_no_fires': (['ch4', 'no'], ['fire']),\n            'CO2_sum': (['co2'], []),\n            'SO2_anthro_fires': (['so2', 'anthro'], ['fire']),\n            'SO2_no_fires': (['so2', 'no'], ['fire'])\n        }\n        \n        if variable in patterns:\n            req, opt = patterns[variable]\n            has_req = all(p in f for p in req)\n            if 'no' in req:\n                has_req = has_req and 'anthro' not in f\n            if opt:\n                return has_req and any(p in f for p in opt)\n            return has_req\n        return False\n    \n    def _is_file_readable(self, file_path: str) -> bool:\n        \"\"\"Check if file can be opened - CRITICAL FIX\"\"\"\n        try:\n            if not os.path.exists(file_path) or os.path.getsize(file_path) < 1000:\n                return False\n            # Quick check without fully loading\n            with xr.open_dataset(file_path, decode_times=False) as ds:\n                _ = list(ds.data_vars.keys())\n            return True\n        except:\n            return False\n    \n    def discover_files_multi_scenario(self, institution: str):\n        \"\"\"Discover files for all scenarios - FIXED with better logging\"\"\"\n        logger.info(f\"🔍 Discovering files for {institution}...\")\n        all_scenario_files = {}\n        \n        for scenario in self.scenarios.keys():\n            logger.info(f\"   📁 {scenario}\")\n            results_inputs = {}\n            results_outputs = {}\n            \n            # Input files\n            for var in self.input_variables:\n                folder = self.variable_dirs.get(var, var)\n                path = os.path.join(self.input_base, scenario, folder)\n                \n                if not os.path.exists(path):\n                    logger.warning(f\"      ⚠️ Path not found: {path}\")\n                    continue\n                \n                found = []\n                try:\n                    for root, dirs, files in os.walk(path):\n                        for fname in files:\n                            if fname.endswith('.nc') and self._matches_variable_pattern(fname, var):\n                                fpath = os.path.join(root, fname)\n                                if self._is_file_readable(fpath):\n                                    found.append(fpath)\n                                    if len(found) % 10 == 0:  # Progress update\n                                        logger.info(f\"         Found {len(found)} {var} files...\")\n                except Exception as e:\n                    logger.error(f\"      ❌ Error walking {path}: {e}\")\n                    continue\n                \n                if found:\n                    results_inputs[var] = found\n                    logger.info(f\"      ✓ {var}: {len(found)} files\")\n                else:\n                    logger.warning(f\"      ⚠️ {var}: No readable files found\")\n            \n            # Output files\n            out_path = os.path.join(self.output_base, institution, scenario, self.output_variable)\n            if os.path.exists(out_path):\n                out_files = []\n                try:\n                    for root, dirs, files in os.walk(out_path):\n                        for fname in files:\n                            if fname.endswith('.nc'):\n                                fpath = os.path.join(root, fname)\n                                if self._is_file_readable(fpath):\n                                    out_files.append(fpath)\n                                    if len(out_files) % 10 == 0:\n                                        logger.info(f\"         Found {len(out_files)} output files...\")\n                except Exception as e:\n                    logger.error(f\"      ❌ Error walking {out_path}: {e}\")\n                \n                if out_files:\n                    results_outputs[self.output_variable] = out_files\n                    logger.info(f\"      ✓ {self.output_variable}: {len(out_files)} files\")\n            else:\n                logger.warning(f\"      ⚠️ Output path not found: {out_path}\")\n            \n            all_scenario_files[scenario] = {\"inputs\": results_inputs, \"outputs\": results_outputs}\n        \n        return all_scenario_files\n    \n    def downsample_spatial(self, arr, target_h, target_w):\n        \"\"\"Downsample using scipy.ndimage.zoom\"\"\"\n        if arr.shape[1] == target_h and arr.shape[2] == target_w:\n            return arr\n        T, H, W = arr.shape\n        downsampled = zoom(arr, [1.0, target_h/H, target_w/W], order=1, mode='nearest')\n        return downsampled[:, :target_h, :target_w]\n    \n    def _load_netcdf_list(self, paths, var_hint=None):\n        \"\"\"Load and concatenate NetCDF files\"\"\"\n        arrays = []\n        for p in sorted(paths):\n            try:\n                ds = xr.open_dataset(p, decode_times=False)\n                dvars = list(ds.data_vars.keys())\n                if not dvars:\n                    ds.close()\n                    continue\n                var = var_hint if (var_hint and var_hint in dvars) else dvars[0]\n                arr = ds[var].values\n                ds.close()\n                if arr.ndim == 2:\n                    arr = np.expand_dims(arr, 0)\n                elif arr.ndim > 3:\n                    arr = arr.reshape(-1, arr.shape[-2], arr.shape[-1])\n                arrays.append(np.nan_to_num(arr, 0.0, 0.0, 0.0))\n            except Exception as e:\n                logger.warning(f\"⚠️ Error reading {os.path.basename(p)}: {e}\")\n        \n        if not arrays:\n            return np.zeros((0, 0, 0), dtype=float)\n        \n        try:\n            return np.concatenate(arrays, axis=0)\n        except:\n            mh = max(a.shape[1] for a in arrays)\n            mw = max(a.shape[2] for a in arrays)\n            padded = [np.pad(a, ((0,0), (0,mh-a.shape[1]), (0,mw-a.shape[2])), 'edge') for a in arrays]\n            return np.concatenate(padded, axis=0)\n    \n    def align_temporal_dimensions(self, var_data):\n        \"\"\"Align temporal dimensions\"\"\"\n        if not var_data:\n            return var_data\n        times = {v: a.shape[0] for v, a in var_data.items()}\n        mt = min(times.values())\n        if mt != max(times.values()):\n            logger.warning(f\"⚠️ Aligning to {mt} timesteps\")\n            return {v: a[:mt] for v, a in var_data.items()}\n        return var_data\n    \n    def load_all_scenarios(self, all_files):\n        \"\"\"Load all scenario data\"\"\"\n        logger.info(\"📊 Loading scenarios...\")\n        all_data = {}\n        \n        for scenario, files in all_files.items():\n            logger.info(f\"   📁 {scenario}\")\n            var_data = {}\n            \n            for var, paths in files.get(\"inputs\", {}).items():\n                if paths:\n                    logger.info(f\"      Loading {var} ({len(paths)} files)...\")\n                    arr = self._load_netcdf_list(paths)\n                    if arr.size > 0:\n                        var_data[var] = arr\n                        logger.info(f\"      ✓ {var}: {arr.shape}\")\n            \n            for var, paths in files.get(\"outputs\", {}).items():\n                if paths:\n                    logger.info(f\"      Loading {var} ({len(paths)} files)...\")\n                    arr = self._load_netcdf_list(paths)\n                    if arr.size > 0:\n                        var_data[var] = arr\n                        logger.info(f\"      ✓ {var}: {arr.shape}\")\n            \n            if var_data:\n                var_data = self.align_temporal_dimensions(var_data)\n                logger.info(f\"      🔽 Downsampling...\")\n                down_data = {}\n                for v, a in var_data.items():\n                    d = self.downsample_spatial(a, self.target_h, self.target_w)\n                    down_data[v] = d\n                    logger.info(f\"         {v}: {a.shape} → {d.shape}\")\n                all_data[scenario] = down_data\n        \n        logger.info(f\"   ✅ Loaded {len(all_data)} scenarios\")\n        return all_data\n    \n    def normalize_data(self, arr, var_name, fit=True):\n        \"\"\"Normalize data\"\"\"\n        arr = np.nan_to_num(arr, 0.0, 0.0, 0.0)\n        flat = arr.reshape(-1, 1)\n        \n        if fit or var_name not in self.scalers:\n            scaler = MinMaxScaler()\n            try:\n                scaler.fit(flat)\n            except:\n                scaler.min_, scaler.scale_ = np.min(flat), 1.0\n            self.scalers[var_name] = scaler\n        else:\n            scaler = self.scalers[var_name]\n        \n        return np.nan_to_num(scaler.transform(flat).reshape(arr.shape), 0.0, 0.0, 0.0)\n    \n    def create_multi_scenario_sequences(self, all_data, seq_in=12, seq_out=3, stride=1, train_ratio=0.7, val_ratio=0.15, batch_size=1):\n        \"\"\"Create sequences with stratified splitting\"\"\"\n        logger.info(\"🔄 Creating sequences...\")\n        X_seqs, Y_seqs, sc_ids = [], [], []\n        \n        for scenario, var_data in all_data.items():\n            if not var_data:\n                continue\n            \n            sc_id = self.scenarios[scenario]\n            logger.info(f\"   📦 {scenario} (id={sc_id})\")\n            \n            missing = [v for v in self.input_variables if v not in var_data]\n            if missing or self.output_variable not in var_data:\n                logger.warning(f\"      ⚠️ Skipping - missing data\")\n                continue\n            \n            # Normalize\n            norm = {v: self.normalize_data(var_data[v], v, True) for v in self.input_variables if v in var_data}\n            norm[self.output_variable] = self.normalize_data(var_data[self.output_variable], self.output_variable, True)\n            \n            # Stack\n            X_sc = np.stack([norm[v] for v in self.input_variables if v in norm], axis=1)\n            Y_sc = norm[self.output_variable]\n            \n            T = X_sc.shape[0]\n            n_samp = T - seq_in - seq_out + 1\n            \n            if n_samp <= 0:\n                logger.warning(f\"      ⚠️ Not enough timesteps\")\n                continue\n            \n            for start in range(0, n_samp, stride):\n                X_seqs.append(X_sc[start:start+seq_in])\n                Y_seqs.append(Y_sc[start+seq_in:start+seq_in+seq_out])\n                sc_ids.append(sc_id)\n            \n            logger.info(f\"      ✓ {n_samp} sequences\")\n        \n        if not X_seqs:\n            raise RuntimeError(\"❌ No sequences!\")\n        \n        X_all = np.stack(X_seqs, 0)\n        Y_all = np.stack(Y_seqs, 0)\n        sc_all = np.array(sc_ids)\n        \n        # Add scenario channel\n        N, T, C, H, W = X_all.shape\n        sc_ch = np.repeat(sc_all[:, None, None, None, None], T*H*W, 1).reshape(N, T, 1, H, W)\n        X_all = np.concatenate([X_all, sc_ch], 2)\n        Y_all = np.expand_dims(Y_all, 2)\n        \n        logger.info(f\"   🧩 Total: X={X_all.shape}, Y={Y_all.shape}\")\n        \n        # Stratified split\n        logger.info(\"   📊 Stratified split...\")\n        X_tr, X_tmp, y_tr, y_tmp, sc_tr, sc_tmp = train_test_split(\n            X_all, Y_all, sc_all, test_size=(1-train_ratio), stratify=sc_all, random_state=42\n        )\n        vt_ratio = val_ratio / (1 - train_ratio)\n        X_val, X_te, y_val, y_te, sc_val, sc_te = train_test_split(\n            X_tmp, y_tmp, sc_tmp, test_size=(1-vt_ratio), stratify=sc_tmp, random_state=42\n        )\n        \n        splits = {\n            \"train\": {\"input\": X_tr, \"target\": y_tr, \"scenarios\": sc_tr},\n            \"validation\": {\"input\": X_val, \"target\": y_val, \"scenarios\": sc_val},\n            \"test\": {\"input\": X_te, \"target\": y_te, \"scenarios\": sc_te}\n        }\n        \n        # CRITICAL FIX: pin_memory=False for Kaggle\n        loaders = {\n            \"train\": DataLoader(ClimateDatasetWithScenario(X_tr, y_tr, sc_tr), batch_size, True, pin_memory=False, num_workers=0),\n            \"validation\": DataLoader(ClimateDatasetWithScenario(X_val, y_val, sc_val), batch_size, False, pin_memory=False, num_workers=0),\n            \"test\": DataLoader(ClimateDatasetWithScenario(X_te, y_te, sc_te), batch_size, False, pin_memory=False, num_workers=0)\n        }\n        \n        logger.info(f\"   📦 Train:{len(X_tr)} Val:{len(X_val)} Test:{len(X_te)}\")\n        return splits, loaders\n\n\nprint(\"✅ Fixed DataLoader loaded\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-09T09:58:13.711345Z","iopub.execute_input":"2025-10-09T09:58:13.711806Z","iopub.status.idle":"2025-10-09T09:58:13.748074Z","shell.execute_reply.started":"2025-10-09T09:58:13.711789Z","shell.execute_reply":"2025-10-09T09:58:13.747361Z"}},"outputs":[{"name":"stdout","text":"✅ Fixed DataLoader loaded\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"## 🧠 Section 4: ClimAx Model (Memory-Efficient)","metadata":{}},{"cell_type":"code","source":"class MemoryEfficientPatchEmbedding(nn.Module):\n    \"\"\"Memory-efficient patch embedding with conv2d.\"\"\"\n    \n    def __init__(self, patch_size: int, in_channels: int, embed_dim: int, \n                 spatial_h: int, spatial_w: int):\n        super().__init__()\n        self.patch_size = patch_size\n        self.in_channels = in_channels\n        self.embed_dim = embed_dim\n        self.spatial_h = spatial_h\n        self.spatial_w = spatial_w\n        \n        # Projection layer\n        self.proj = nn.Conv2d(in_channels, embed_dim, \n                             kernel_size=patch_size, stride=patch_size)\n        \n        # Calculate number of patches\n        self.num_patches_h = spatial_h // patch_size\n        self.num_patches_w = spatial_w // patch_size\n        self.num_patches = self.num_patches_h * self.num_patches_w\n        \n        logger.info(f\"🔧 PatchEmbed: {spatial_h}×{spatial_w} → {self.num_patches_h}×{self.num_patches_w} = {self.num_patches} patches\")\n        \n    def forward(self, x):\n        # x: (B, T, C, H, W)\n        B, T, C, H, W = x.shape\n        \n        # Verify dimensions\n        if H != self.spatial_h or W != self.spatial_w:\n            raise ValueError(f\"Input spatial dims ({H}, {W}) don't match expected ({self.spatial_h}, {self.spatial_w})\")\n        \n        # Process in chunks to save memory\n        x = x.reshape(B * T, C, H, W)\n        x = self.proj(x)  # (B*T, embed_dim, H', W')\n        x = x.flatten(2).transpose(1, 2)  # (B*T, num_patches, embed_dim)\n        x = x.reshape(B, T, self.num_patches, self.embed_dim)\n        \n        return x\n\n\nclass MemoryEfficientPositionalEmbedding(nn.Module):\n    \"\"\"Memory-efficient positional embeddings.\"\"\"\n    \n    def __init__(self, num_patches: int, num_timesteps: int, embed_dim: int):\n        super().__init__()\n        self.num_patches = num_patches\n        self.num_timesteps = num_timesteps\n        self.embed_dim = embed_dim\n        \n        # Use smaller embeddings\n        self.spatial_pos_embed = nn.Parameter(torch.zeros(1, 1, num_patches, embed_dim))\n        self.temporal_pos_embed = nn.Parameter(torch.zeros(1, num_timesteps, 1, embed_dim))\n        \n        # Initialize with smaller std\n        nn.init.trunc_normal_(self.spatial_pos_embed, std=0.01)\n        nn.init.trunc_normal_(self.temporal_pos_embed, std=0.01)\n    \n    def forward(self, x):\n        B, T, P, D = x.shape\n        \n        if P != self.num_patches:\n            raise ValueError(f\"Number of patches mismatch: got {P}, expected {self.num_patches}\")\n        if T > self.num_timesteps:\n            raise ValueError(f\"Timesteps exceed maximum: got {T}, max {self.num_timesteps}\")\n        \n        return x + self.spatial_pos_embed + self.temporal_pos_embed[:, :T, :, :]\n\n\nclass MemoryEfficientTransformerBlock(nn.Module):\n    \"\"\"Memory-efficient Transformer block with gradient checkpointing.\"\"\"\n    \n    def __init__(self, embed_dim: int, num_heads: int, mlp_ratio: float = 4.0,\n                 dropout: float = 0.1, attention_dropout: float = 0.1):\n        super().__init__()\n        \n        self.norm1 = nn.LayerNorm(embed_dim)\n        self.attn = nn.MultiheadAttention(\n            embed_dim, num_heads, \n            dropout=attention_dropout, \n            batch_first=True\n        )\n        \n        self.norm2 = nn.LayerNorm(embed_dim)\n        mlp_hidden_dim = int(embed_dim * mlp_ratio)\n        self.mlp = nn.Sequential(\n            nn.Linear(embed_dim, mlp_hidden_dim),\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.Linear(mlp_hidden_dim, embed_dim),\n            nn.Dropout(dropout)\n        )\n    \n    def forward(self, x):\n        # Self-attention with residual\n        normed = self.norm1(x)\n        attn_out, _ = self.attn(normed, normed, normed)\n        x = x + attn_out\n        \n        # FFN with residual\n        x = x + self.mlp(self.norm2(x))\n        \n        return x\n\n\nclass ClimAxMemoryEfficientModel(nn.Module):\n    \"\"\"\n    ClimAx Model - MEMORY EFFICIENT VERSION\n    \n    ✅ Designed for 6GB GPU\n    ✅ Uses ConvLSTM spatial dimensions (9×19)\n    ✅ Gradient checkpointing enabled\n    ✅ Optimized attention mechanisms\n    ✅ Supports NUM_INPUT_CHANNELS for scenario embeddings\n    \"\"\"\n    \n    def __init__(self, config, spatial_h: Optional[int] = None, \n                 spatial_w: Optional[int] = None, use_checkpointing: bool = True):\n        super().__init__()\n        \n        # Get spatial dims - prioritize 9×19 for memory efficiency\n        if spatial_h is not None and spatial_w is not None:\n            self.spatial_h = spatial_h\n            self.spatial_w = spatial_w\n        else:\n            # Default to ConvLSTM's memory-efficient dimensions\n            self.spatial_h = self._get_config_attr(config, 'SPATIAL_HEIGHT', 9)\n            self.spatial_w = self._get_config_attr(config, 'SPATIAL_WIDTH', 19)\n        \n        # CRITICAL: Warn if dimensions are too large\n        total_spatial = self.spatial_h * self.spatial_w\n        if total_spatial > 500:\n            logger.warning(f\"⚠️  Large spatial dimensions ({self.spatial_h}×{self.spatial_w} = {total_spatial}) may cause OOM!\")\n            logger.warning(f\"   Consider downsampling to 9×19 (171 pixels) like ConvLSTM\")\n        \n        # Get model params\n        self.patch_size = self._get_config_attr(config, 'PATCH_SIZE', 2)\n        self.embed_dim = self._get_config_attr(config, 'EMBED_DIM', 128)\n        self.depth = self._get_config_attr(config, 'DEPTH', 8)\n        self.num_heads = self._get_config_attr(config, 'NUM_HEADS', 8)\n        self.mlp_ratio = self._get_config_attr(config, 'MLP_RATIO', 4.0)\n        self.dropout_rate = self._get_config_attr(config, 'DROPOUT_RATE', 0.1)\n        self.attention_dropout = self._get_config_attr(config, 'ATTENTION_DROPOUT', 0.1)\n        \n        # Reduce model size if needed\n        if total_spatial > 300:\n            logger.warning(\"   → Reducing embed_dim from 128 to 64 to save memory\")\n            self.embed_dim = 64\n            logger.warning(\"   → Reducing depth from 8 to 4 to save memory\")\n            self.depth = 4\n        \n        # ✅ FIXED: Input channels - prioritize NUM_INPUT_CHANNELS\n        self.num_input_vars = self._get_config_attr(config, 'NUM_INPUT_CHANNELS', None)\n        if self.num_input_vars is None:\n            # Fall back to counting INPUT_VARIABLES\n            input_vars = self._get_config_attr(config, 'INPUT_VARIABLES', [])\n            self.num_input_vars = len(input_vars) if input_vars else 4\n            logger.info(f\"   📊 Input channels from INPUT_VARIABLES: {self.num_input_vars}\")\n        else:\n            logger.info(f\"   📊 Input channels from NUM_INPUT_CHANNELS: {self.num_input_vars}\")\n        \n        self.seq_in_len = self._get_config_attr(config, 'SEQUENCE_INPUT_LENGTH', 12)\n        self.seq_out_len = self._get_config_attr(config, 'SEQUENCE_OUTPUT_LENGTH', 3)\n        \n        # Calculate patches\n        self.num_patches_h = self.spatial_h // self.patch_size\n        self.num_patches_w = self.spatial_w // self.patch_size\n        self.num_patches = self.num_patches_h * self.num_patches_w\n        \n        # Memory estimation\n        tokens_per_batch = self.num_patches * self.seq_in_len\n        attn_memory_gb = (tokens_per_batch ** 2 * 4) / (1024 ** 3)  # float32 bytes to GB\n        \n        logger.info(f\"🔧 Memory-Efficient ClimAx initialized:\")\n        logger.info(f\"   Spatial: {self.spatial_h}×{self.spatial_w} → {self.num_patches} patches\")\n        logger.info(f\"   Tokens per sample: {tokens_per_batch}\")\n        logger.info(f\"   Est. attention memory: {attn_memory_gb:.2f} GB per batch\")\n        logger.info(f\"   Embed dim: {self.embed_dim}, Depth: {self.depth}\")\n        logger.info(f\"   Gradient checkpointing: {use_checkpointing}\")\n        \n        if attn_memory_gb > 4:\n            logger.error(f\"❌ Estimated memory ({attn_memory_gb:.2f} GB) exceeds safe limit!\")\n            logger.error(f\"   Please reduce spatial dimensions or batch size\")\n            raise RuntimeError(f\"Model too large for 6GB GPU! Need ~{attn_memory_gb:.1f}GB\")\n        \n        self.use_checkpointing = use_checkpointing\n        \n        # Patch embedding\n        self.patch_embed = MemoryEfficientPatchEmbedding(\n            patch_size=self.patch_size,\n            in_channels=self.num_input_vars,\n            embed_dim=self.embed_dim,\n            spatial_h=self.spatial_h,\n            spatial_w=self.spatial_w\n        )\n        \n        # Positional embeddings\n        self.pos_embed = MemoryEfficientPositionalEmbedding(\n            num_patches=self.num_patches,\n            num_timesteps=self.seq_in_len,\n            embed_dim=self.embed_dim\n        )\n        \n        # Transformer blocks\n        self.blocks = nn.ModuleList([\n            MemoryEfficientTransformerBlock(\n                embed_dim=self.embed_dim,\n                num_heads=self.num_heads,\n                mlp_ratio=self.mlp_ratio,\n                dropout=self.dropout_rate,\n                attention_dropout=self.attention_dropout\n            ) for _ in range(self.depth)\n        ])\n        \n        self.norm = nn.LayerNorm(self.embed_dim)\n        \n        # Lightweight prediction head\n        self.head = nn.Sequential(\n            nn.Linear(self.embed_dim, self.embed_dim),\n            nn.GELU(),\n            nn.Dropout(self.dropout_rate),\n            nn.Linear(self.embed_dim, self.patch_size * self.patch_size * self.seq_out_len)\n        )\n        \n        # Count parameters\n        self.num_params = sum(p.numel() for p in self.parameters())\n        logger.info(f\"✅ Model ready - Parameters: {self.num_params:,}\")\n    \n    def _get_config_attr(self, config, attr_name, default):\n        \"\"\"Safely get attribute from config.\"\"\"\n        if hasattr(config, attr_name):\n            return getattr(config, attr_name)\n        \n        for ns in ['data', 'model', 'training']:\n            if hasattr(config, ns):\n                ns_obj = getattr(config, ns)\n                if ns_obj is not None and hasattr(ns_obj, attr_name):\n                    return getattr(ns_obj, attr_name)\n        \n        return default\n    \n    def forward(self, x):\n        \"\"\"\n        Memory-efficient forward pass with gradient checkpointing.\n        \n        Args:\n            x: (B, T, C, H, W) input tensor\n        \n        Returns:\n            (B, T_out, H, W) predictions\n        \"\"\"\n        B, T, C, H, W = x.shape\n        \n        # Store original spatial dims for reconstruction\n        original_h, original_w = H, W\n        \n        # Verify dimensions match\n        if H != self.spatial_h or W != self.spatial_w:\n            raise ValueError(\n                f\"Input spatial dims ({H}, {W}) don't match model ({self.spatial_h}, {self.spatial_w}). \"\n                f\"Please downsample your data to {self.spatial_h}×{self.spatial_w} before training.\"\n            )\n        \n        # Patch embedding\n        x = self.patch_embed(x)  # (B, T, num_patches, embed_dim)\n        \n        # Add positional embeddings\n        x = self.pos_embed(x)\n        \n        # Flatten for transformer\n        B, T, P, D = x.shape\n        x = x.reshape(B, T * P, D)\n        \n        # Apply transformer blocks with gradient checkpointing\n        for i, block in enumerate(self.blocks):\n            if self.use_checkpointing and self.training:\n                x = checkpoint(block, x, use_reentrant=False)\n            else:\n                x = block(x)\n        \n        x = self.norm(x)\n        \n        # Prediction head\n        x = self.head(x)  # (B, T*P, patch_size^2 * seq_out_len)\n        \n        # Reshape to spatial output\n        x = x.reshape(B, T, P, self.patch_size, self.patch_size, self.seq_out_len)\n        \n        # Reorganize patches back to spatial grid\n        x = x.reshape(B, T, self.num_patches_h, self.num_patches_w, \n                     self.patch_size, self.patch_size, self.seq_out_len)\n        \n        # Merge patches: (B, T, seq_out_len, H_recon, W_recon)\n        x = x.permute(0, 1, 6, 2, 4, 3, 5).contiguous()\n        \n        # Calculate reconstructed dimensions\n        h_recon = self.num_patches_h * self.patch_size\n        w_recon = self.num_patches_w * self.patch_size\n        \n        x = x.reshape(B, T, self.seq_out_len, h_recon, w_recon)\n        \n        # Upsample if needed to match original dimensions\n        if h_recon != original_h or w_recon != original_w:\n            x = x.reshape(B * T * self.seq_out_len, 1, h_recon, w_recon)\n            x = F.interpolate(x, size=(original_h, original_w), mode='bilinear', align_corners=False)\n            x = x.reshape(B, T, self.seq_out_len, original_h, original_w)\n        \n        # Average over input timesteps\n        x = x.mean(dim=1)  # (B, seq_out_len, H, W)\n        \n        return x\n\n\nprint(\"✅ ClimAxMemoryEfficientModel class defined\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-09T09:58:13.748725Z","iopub.execute_input":"2025-10-09T09:58:13.748909Z","iopub.status.idle":"2025-10-09T09:58:13.776215Z","shell.execute_reply.started":"2025-10-09T09:58:13.748895Z","shell.execute_reply":"2025-10-09T09:58:13.775512Z"}},"outputs":[{"name":"stdout","text":"✅ ClimAxMemoryEfficientModel class defined\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"## 🚂 Section 5: Training Function","metadata":{}},{"cell_type":"code","source":"# =============================================================================\n# 🚂 SECTION 5: TRAINING FUNCTION - FIXED WITH DEBUG LOGGING\n# =============================================================================\n\ndef train_single_institution_multi_scenario(institution: str, config: Phase4Config) -> dict:\n    \"\"\"Train a single institution - FIXED with extensive debug logging\"\"\"\n    \n    print(f\"\\n{'='*80}\")\n    print(f\"🌍 Training: {institution}\")\n    print(f\"{'='*80}\")\n    \n    start_time = time.time()\n    \n    # Check for existing model\n    model_path = os.path.join(OUTPUT_DIR, \"checkpoints\", f\"{institution}_multiscenario_best.pt\")\n    \n    if RESUME_IF_MODEL_EXISTS and os.path.exists(model_path):\n        print(f\"⭐ Model exists - skipping {institution}\")\n        return {'success': True, 'institution': institution, 'skipped': True}\n    \n    try:\n        # STEP 1: Create data loader\n        print(\"📊 Step 1/8: Initializing data loader...\")\n        sys.stdout.flush()\n        dl = Phase4MultiScenarioDataLoader(config, target_h=TARGET_SPATIAL_H, target_w=TARGET_SPATIAL_W)\n        print(\"✅ Data loader created\")\n        sys.stdout.flush()\n        \n        # STEP 2: Discover files\n        print(\"🔍 Step 2/8: Discovering files...\")\n        sys.stdout.flush()\n        all_scenario_files = dl.discover_files_multi_scenario(institution)\n        \n        has_data = any(\n            bool(files['inputs']) or bool(files['outputs'])\n            for files in all_scenario_files.values()\n        )\n        \n        if not has_data:\n            print(f\"⚠️ No data for {institution}\")\n            return {'success': False, 'institution': institution, 'error': 'No data'}\n        \n        print(\"✅ File discovery complete\")\n        sys.stdout.flush()\n        \n        # STEP 3: Load all scenarios\n        print(\"📊 Step 3/8: Loading data...\")\n        sys.stdout.flush()\n        all_scenario_data = dl.load_all_scenarios(all_scenario_files)\n        \n        if not all_scenario_data:\n            print(f\"⚠️ Failed to load data\")\n            return {'success': False, 'institution': institution, 'error': 'Load failed'}\n        \n        print(\"✅ Data loaded\")\n        sys.stdout.flush()\n        \n        # STEP 4: Create sequences\n        print(\"🔄 Step 4/8: Creating sequences...\")\n        sys.stdout.flush()\n        data_splits, dataloaders = dl.create_multi_scenario_sequences(all_scenario_data)\n        print(\"✅ Sequences created\")\n        sys.stdout.flush()\n        \n        # STEP 5: Create model\n        print(\"🧠 Step 5/8: Creating model...\")\n        sys.stdout.flush()\n        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n        print(f\"   Using device: {device}\")\n        \n        model = ClimAxMemoryEfficientModel(\n            config,\n            spatial_h=TARGET_SPATIAL_H,\n            spatial_w=TARGET_SPATIAL_W,\n            use_checkpointing=USE_CHECKPOINTING\n        ).to(device)\n        \n        print(\"✅ Model created\")\n        sys.stdout.flush()\n        \n        # STEP 6: Setup training\n        print(\"⚙️ Step 6/8: Setting up optimizer...\")\n        sys.stdout.flush()\n        optimizer = optim.AdamW(\n            model.parameters(),\n            lr=config.LEARNING_RATE,\n            weight_decay=config.WEIGHT_DECAY\n        )\n        criterion = nn.MSELoss()\n        \n        history = {\n            'train_loss': [],\n            'val_loss': [],\n            'scenario_losses': {}\n        }\n        best_val_loss = float('inf')\n        \n        print(\"✅ Optimizer ready\")\n        sys.stdout.flush()\n        \n        print(f\"\\n🚂 Step 7/8: Training for {EPOCHS} epochs...\")\n        sys.stdout.flush()\n        \n        # STEP 7: Training loop with progress bars\n        epoch_pbar = tqdm(range(EPOCHS), desc=f\"🌍 {institution}\", position=0, leave=True)\n        \n        for ep in epoch_pbar:\n            print(f\"\\n--- Epoch {ep+1}/{EPOCHS} ---\")\n            sys.stdout.flush()\n            \n            # === TRAINING PHASE ===\n            model.train()\n            train_loss = 0.0\n            train_batches = 0\n            \n            print(f\"Training: Processing {len(dataloaders['train'])} batches...\")\n            sys.stdout.flush()\n            \n            train_pbar = tqdm(\n                dataloaders['train'], \n                desc=f\"   Train Epoch {ep+1}/{EPOCHS}\",\n                position=1,\n                leave=False,\n                bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}]'\n            )\n            \n            for batch_idx, batch in enumerate(train_pbar):\n                try:\n                    X, y, scenarios = batch\n                    X = X.to(device)\n                    y = y.to(device)\n                    \n                    optimizer.zero_grad()\n                    preds = model(X)\n                    \n                    # Align shapes\n                    if preds.shape != y.shape:\n                        if preds.ndim == 4 and y.ndim == 5:\n                            preds = preds.unsqueeze(2)\n                    \n                    loss = criterion(preds, y)\n                    loss.backward()\n                    \n                    torch.nn.utils.clip_grad_norm_(model.parameters(), config.CLIP_GRAD_NORM)\n                    optimizer.step()\n                    \n                    train_loss += loss.item()\n                    train_batches += 1\n                    \n                    # Update progress bar\n                    train_pbar.set_postfix({'loss': f'{loss.item():.6f}'})\n                    \n                    # Debug print every 5 batches\n                    if batch_idx % 5 == 0:\n                        print(f\"      Batch {batch_idx}: loss={loss.item():.6f}\")\n                        sys.stdout.flush()\n                    \n                    if train_batches % 10 == 0:\n                        torch.cuda.empty_cache()\n                \n                except Exception as e:\n                    print(f\"\\n❌ Error in training batch {batch_idx}: {e}\")\n                    import traceback\n                    traceback.print_exc()\n                    raise\n            \n            avg_train_loss = train_loss / max(1, train_batches)\n            history['train_loss'].append(avg_train_loss)\n            \n            print(f\"   ✓ Train loss: {avg_train_loss:.6f}\")\n            sys.stdout.flush()\n            \n            # === VALIDATION PHASE ===\n            model.eval()\n            val_loss = 0.0\n            val_batches = 0\n            scenario_losses = {s: [] for s in SCENARIOS.keys()}\n            \n            print(f\"Validation: Processing {len(dataloaders['validation'])} batches...\")\n            sys.stdout.flush()\n            \n            val_pbar = tqdm(\n                dataloaders['validation'],\n                desc=f\"   Val Epoch {ep+1}/{EPOCHS}\",\n                position=1,\n                leave=False,\n                bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt}'\n            )\n            \n            with torch.no_grad():\n                for batch_idx, batch in enumerate(val_pbar):\n                    try:\n                        Xv, yv, scenarios_v = batch\n                        Xv = Xv.to(device)\n                        yv = yv.to(device)\n                        \n                        preds = model(Xv)\n                        \n                        if preds.shape != yv.shape:\n                            if preds.ndim == 4 and yv.ndim == 5:\n                                preds = preds.unsqueeze(2)\n                        \n                        batch_loss = criterion(preds, yv)\n                        val_loss += batch_loss.item()\n                        val_batches += 1\n                        \n                        # Track per-scenario losses\n                        for i, scenario_id in enumerate(scenarios_v.cpu().numpy()):\n                            scenario_name = [k for k, v in SCENARIOS.items() if v == scenario_id][0]\n                            scenario_losses[scenario_name].append(batch_loss.item())\n                        \n                        val_pbar.set_postfix({'loss': f'{batch_loss.item():.6f}'})\n                    \n                    except Exception as e:\n                        print(f\"\\n❌ Error in validation batch {batch_idx}: {e}\")\n                        import traceback\n                        traceback.print_exc()\n                        raise\n            \n            avg_val_loss = val_loss / max(1, val_batches)\n            history['val_loss'].append(avg_val_loss)\n            \n            print(f\"   ✓ Val loss: {avg_val_loss:.6f}\")\n            sys.stdout.flush()\n            \n            # Calculate per-scenario averages\n            avg_scenario_losses = {\n                s: np.mean(losses) if losses else 0.0\n                for s, losses in scenario_losses.items()\n            }\n            history['scenario_losses'][f'epoch_{ep+1}'] = avg_scenario_losses\n            \n            # Update best model\n            is_best = avg_val_loss < best_val_loss\n            if is_best:\n                best_val_loss = avg_val_loss\n                best_indicator = \"⭐ NEW BEST\"\n            else:\n                best_indicator = \"\"\n            \n            # Update main progress bar\n            epoch_pbar.set_postfix({\n                'train': f'{avg_train_loss:.6f}',\n                'val': f'{avg_val_loss:.6f}',\n                'best': f'{best_val_loss:.6f}',\n                'status': best_indicator\n            })\n            \n            # Memory cleanup\n            torch.cuda.empty_cache()\n            gc.collect()\n        \n        # Close progress bars\n        epoch_pbar.close()\n        \n        # STEP 8: Save model\n        print(\"💾 Step 8/8: Saving model...\")\n        sys.stdout.flush()\n        torch.save({\n            'institution': institution,\n            'model_state_dict': model.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            'history': history,\n            'best_val_loss': best_val_loss,\n            'spatial_dims': [TARGET_SPATIAL_H, TARGET_SPATIAL_W],\n            'scenarios': list(SCENARIOS.keys()),\n            'timestamp': datetime.now().isoformat()\n        }, model_path)\n        \n        print(f\"✅ Saved to {model_path}\")\n        sys.stdout.flush()\n        \n        # Save results\n        summary = {\n            'institution': institution,\n            'training_time': time.time() - start_time,\n            'epochs_trained': len(history['train_loss']),\n            'best_val_loss': best_val_loss,\n            'spatial_dims': [TARGET_SPATIAL_H, TARGET_SPATIAL_W],\n            'scenarios': list(SCENARIOS.keys()),\n            'final_scenario_losses': history['scenario_losses'].get(f'epoch_{EPOCHS}', {}),\n            'timestamp': datetime.now().isoformat()\n        }\n        \n        results_path = os.path.join(OUTPUT_DIR, \"logs\", f\"{institution}_phase4_results.json\")\n        with open(results_path, 'w') as f:\n            json.dump(summary, f, indent=2)\n        \n        return {'success': True, 'institution': institution, 'results': summary}\n    \n    except Exception as e:\n        print(f\"\\n❌ FATAL ERROR: {e}\")\n        import traceback\n        traceback.print_exc()\n        sys.stdout.flush()\n        return {'success': False, 'institution': institution, 'error': str(e)}\n    \n    finally:\n        torch.cuda.empty_cache()\n        gc.collect()\n\n\nprint(\"✅ Fixed training function loaded\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-09T09:58:13.776918Z","iopub.execute_input":"2025-10-09T09:58:13.777137Z","iopub.status.idle":"2025-10-09T09:58:13.802269Z","shell.execute_reply.started":"2025-10-09T09:58:13.777113Z","shell.execute_reply":"2025-10-09T09:58:13.801587Z"}},"outputs":[{"name":"stdout","text":"✅ Fixed training function loaded\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"## 🚀 Section 6: Main Training Loop","metadata":{}},{"cell_type":"code","source":"# =============================================================================\n# 🚀 SECTION 6: MAIN TRAINING EXECUTION - FIXED WITH DEBUG\n# =============================================================================\n\nprint(\"=\"*80)\nprint(\"🌍 ClimAx Phase 4 - Multi-Scenario Training\")\nprint(\"=\"*80)\nprint(f\"Start time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\nsys.stdout.flush()\n\n# Create config\nconfig = Phase4Config()\n\n# Determine institutions to train\nif SMOKE_TEST:\n    institutions = ALL_INSTITUTIONS[:2]\n    print(f\"⚠️ SMOKE TEST - Training only {len(institutions)} institutions\")\nelse:\n    institutions = ALL_INSTITUTIONS\n    print(f\"📋 Training all {len(institutions)} institutions\")\n\nprint(f\"   Scenarios: {list(SCENARIOS.keys())}\")\nprint(f\"   Spatial: {TARGET_SPATIAL_H}×{TARGET_SPATIAL_W}\")\nprint(f\"   Batch size: {BATCH_SIZE}\")\nprint(f\"   Epochs: {EPOCHS}\")\nprint(f\"   Patch size: {config.PATCH_SIZE}\")\nprint(f\"   Input channels: {config.NUM_INPUT_CHANNELS}\")\nsys.stdout.flush()\n\n# GPU Check\nif torch.cuda.is_available():\n    print(f\"\\n🎮 GPU Status:\")\n    print(f\"   Device: {torch.cuda.get_device_name(0)}\")\n    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / (1024**3):.1f} GB\")\n    print(f\"   Current allocation: {torch.cuda.memory_allocated() / (1024**2):.1f} MB\")\nelse:\n    print(\"\\n⚠️ WARNING: No GPU available - will be VERY slow!\")\nsys.stdout.flush()\n\nif SKIP_TRAINING:\n    print(\"SKIP_TRAINING=True - Loading existing results...\")\n    summary_path = os.path.join(OUTPUT_DIR, \"logs\", \"phase4_training_summary.json\")\n    if os.path.exists(summary_path):\n        with open(summary_path, 'r') as f:\n            summary = json.load(f)\n        print(\"✅ Loaded existing results\")\n    else:\n        print(\"⚠️ No existing results found\")\n        summary = {}\nelse:\n    # Training loop with main progress bar\n    print(\"\\n\" + \"=\"*80)\n    print(\"🚂 STARTING TRAINING\")\n    print(\"=\"*80)\n    sys.stdout.flush()\n    \n    t0 = time.time()\n    all_results = []\n    \n    main_pbar = tqdm(\n        institutions,\n        desc=\"🌍 Overall Progress\",\n        position=0,\n        bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}]'\n    )\n    \n    for inst_idx, institution in enumerate(main_pbar):\n        print(f\"\\n{'='*80}\")\n        print(f\"Institution {inst_idx+1}/{len(institutions)}: {institution}\")\n        print(f\"{'='*80}\")\n        sys.stdout.flush()\n        \n        main_pbar.set_description(f\"🌍 Training {institution}\")\n        \n        try:\n            result = train_single_institution_multi_scenario(institution, config)\n            all_results.append(result)\n            \n            # Update main progress with status\n            successful = sum(1 for r in all_results if r.get('success') and not r.get('skipped'))\n            skipped = sum(1 for r in all_results if r.get('skipped'))\n            failed = sum(1 for r in all_results if not r.get('success'))\n            \n            print(f\"\\n📊 Progress Summary:\")\n            print(f\"   ✅ Successful: {successful}\")\n            print(f\"   ⭐ Skipped: {skipped}\")\n            print(f\"   ❌ Failed: {failed}\")\n            sys.stdout.flush()\n            \n            main_pbar.set_postfix({\n                'success': successful,\n                'skipped': skipped,\n                'failed': failed\n            })\n            \n            # Save progress after each institution\n            progress = {\n                'completed': len(all_results),\n                'total': len(institutions),\n                'results': all_results,\n                'timestamp': datetime.now().isoformat()\n            }\n            \n            progress_path = os.path.join(OUTPUT_DIR, \"logs\", \"phase4_progress.json\")\n            with open(progress_path, 'w') as f:\n                json.dump(progress, f, indent=2)\n            print(f\"💾 Progress saved to {progress_path}\")\n            sys.stdout.flush()\n        \n        except Exception as e:\n            print(f\"\\n❌ CRITICAL ERROR processing {institution}: {e}\")\n            import traceback\n            traceback.print_exc()\n            sys.stdout.flush()\n            \n            all_results.append({\n                'success': False,\n                'institution': institution,\n                'error': str(e)\n            })\n    \n    main_pbar.close()\n    elapsed = time.time() - t0\n    \n    # Compile summary\n    successful = [r for r in all_results if r.get('success') and not r.get('skipped')]\n    skipped = [r for r in all_results if r.get('skipped')]\n    failed = [r for r in all_results if not r.get('success')]\n    \n    summary = {\n        'phase': 'phase4_multi_scenario',\n        'total_institutions': len(institutions),\n        'successful': len(successful),\n        'skipped': len(skipped),\n        'failed': len(failed),\n        'total_time_hours': elapsed / 3600,\n        'scenarios': list(SCENARIOS.keys()),\n        'results': all_results,\n        'timestamp': datetime.now().isoformat()\n    }\n    \n    # Save summary\n    summary_path = os.path.join(OUTPUT_DIR, \"logs\", \"phase4_training_summary.json\")\n    with open(summary_path, 'w') as f:\n        json.dump(summary, f, indent=2)\n    \n    print(f\"\\n{'='*80}\")\n    print(\"✅ Phase 4 Training Complete!\")\n    print(f\"{'='*80}\")\n    print(f\"Total institutions: {len(institutions)}\")\n    print(f\"  ✅ Successful: {len(successful)}\")\n    print(f\"  ⭐ Skipped: {len(skipped)}\")\n    print(f\"  ❌ Failed: {len(failed)}\")\n    print(f\"Total time: {elapsed/3600:.2f} hours\")\n    print(f\"{'='*80}\")\n    sys.stdout.flush()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-09T09:58:13.803095Z","iopub.execute_input":"2025-10-09T09:58:13.803396Z"}},"outputs":[{"name":"stdout","text":"================================================================================\n🌍 ClimAx Phase 4 - Multi-Scenario Training\n================================================================================\nStart time: 2025-10-09 09:58:13\n⚠️ SMOKE TEST - Training only 2 institutions\n   Scenarios: ['historical', 'ssp126', 'ssp245', 'ssp370', 'ssp585']\n   Spatial: 9×19\n   Batch size: 1\n   Epochs: 2\n   Patch size: 1\n   Input channels: 8\n\n🎮 GPU Status:\n   Device: Tesla P100-PCIE-16GB\n   Memory: 15.9 GB\n   Current allocation: 0.0 MB\n\n================================================================================\n🚂 STARTING TRAINING\n================================================================================\n","output_type":"stream"},{"name":"stderr","text":"🌍 Overall Progress:   0%|          | 0/2 [00:00<?]","output_type":"stream"},{"name":"stdout","text":"\n================================================================================\nInstitution 1/2: AWI-CM-1-1-MR\n================================================================================\n","output_type":"stream"},{"name":"stderr","text":"🌍 Training AWI-CM-1-1-MR:   0%|          | 0/2 [00:00<?]","output_type":"stream"},{"name":"stdout","text":"\n================================================================================\n🌍 Training: AWI-CM-1-1-MR\n================================================================================\n📊 Step 1/8: Initializing data loader...\n✅ Data loader created\n🔍 Step 2/8: Discovering files...\n","output_type":"stream"}],"execution_count":null},{"cell_type":"markdown","source":"## 📊 Section 7: Display Results","metadata":{}},{"cell_type":"code","source":"# Display results\nif summary:\n    print(\"\\n\" + \"=\"*80)\n    print(\"📊 PHASE 4 RESULTS SUMMARY\")\n    print(\"=\"*80)\n    print(f\"Total institutions: {summary.get('total_institutions', 0)}\")\n    print(f\"  ✅ Successful: {summary.get('successful', 0)}\")\n    print(f\"  ⭐ Skipped: {summary.get('skipped', 0)}\")\n    print(f\"  ❌ Failed: {summary.get('failed', 0)}\")\n    print(f\"Total time: {summary.get('total_time_hours', 0):.2f}h\")\n    print(f\"Scenarios: {summary.get('scenarios', [])}\")\n    \n    # Scenario performance\n    print(f\"\\n🌍 Scenario Performance Across All Institutions:\")\n    print(\"=\"*80)\n    \n    scenario_perf = {s: [] for s in SCENARIOS.keys()}\n    \n    for result in summary.get('results', []):\n        if result.get('success') and not result.get('skipped'):\n            inst_results = result.get('results', {})\n            final_losses = inst_results.get('final_scenario_losses', {})\n            \n            for scenario, loss in final_losses.items():\n                if scenario in scenario_perf:\n                    scenario_perf[scenario].append(loss)\n    \n    for scenario, losses in scenario_perf.items():\n        if losses:\n            avg = np.mean(losses)\n            std = np.std(losses)\n            min_loss = np.min(losses)\n            max_loss = np.max(losses)\n            print(f\"   {scenario.upper():12s}: Avg={avg:.6f}±{std:.6f} | Min={min_loss:.6f} | Max={max_loss:.6f}\")\n        else:\n            print(f\"   {scenario.upper():12s}: No data\")\n    \n    print(\"=\"*80)\n\nprint(f\"\\n✅ Results saved to: {OUTPUT_DIR}\")\nprint(\"=\"*80)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 🎉 Done!\n\nYour models are now trained and saved in the `climax_phase4_results/` directory.\n\n### 📁 Output Files:\n- **Models**: `climax_phase4_results/checkpoints/*.pt`\n- **Logs**: `climax_phase4_results/logs/*.json`\n- **Summary**: `climax_phase4_results/logs/phase4_training_summary.json`\n\n### 🔄 Next Steps:\n1. Load trained models for inference\n2. Evaluate on test set\n3. Generate predictions for different scenarios\n4. Visualize results","metadata":{}}]}